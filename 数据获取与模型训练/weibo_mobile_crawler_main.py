
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
weibo_mobile_crawler_main.py
ç§»åŠ¨ç«¯å¾®åšçˆ¬è™«ï¼ˆåŸºäº m.weibo.cn æ¥å£ï¼‰ï¼Œä¸é¡¹ç›®ç°æœ‰è¡¨ç»“æ„/é…ç½®å…¼å®¹ã€‚

ç‰¹æ€§
- è¯»å– config.jsonï¼škeywordsã€startPageã€maxPageã€crawler_settingsã€headersã€proxiesã€mysql_config
- æŠ“å–æœç´¢ç»“æœï¼ˆcards.card_type==9 çš„å¾®åšï¼‰ï¼Œè§£ææ–‡æœ¬/å›¾ç‰‡/æ—¶é—´/ç”¨æˆ·/è®¡æ•°
- å¯é€‰æŠ“å–çƒ­è¯„æ¥å£ï¼ˆcomments/hotflowï¼‰
- æ•°æ®åº“ä½¿ç”¨ pymysqlï¼Œå†™å…¥ weibo_data / commentsï¼ˆINSERT ... ON DUPLICATE KEY UPDATEï¼‰
- å…³é”®è¯è‹¥æœªå­˜åœ¨äº keywords è¡¨ï¼Œä¼šè‡ªåŠ¨æ’å…¥å†å– IDï¼ˆæ»¡è¶³å¤–é”®çº¦æŸï¼‰
- æ—¥å¿—æ¸…æ™°ï¼šå…³é”®è¯->åˆ†é¡µ->æŠ“å–æ¡æ•°->å…¥åº“æ¡æ•°ï¼›é”™è¯¯/é™æµæ—¶æ˜ç¡®æç¤º
- å¯é€‰ï¼šå¦‚åŒç›®å½•å­˜åœ¨ db_bootstrap.pyï¼Œåˆ™ä»¥å­è¿›ç¨‹æ–¹å¼æ‰§è¡Œ â€œ--mode initâ€ åšæ— æŸå»ºè¡¨æ ¡éªŒ

å…¼å®¹è¡¨ç»“æ„
- weibo_data(id, keyword_id, text, pics, timestamp, source, user_name, reposts_count, comments_count, attitudes_count, created_at, updated_at)
- comments(id, weibo_id, user, content, timestamp, created_at, updated_at)
- keywords(id, keyword)
"""

import json
import os
import re
import sys
import time
import random
import logging
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

import requests
import pymysql

# ---------------- Logging ----------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("weibo_mobile_crawler.log", encoding="utf-8")
    ]
)
logger = logging.getLogger(__name__)

# ---------------- Config -----------------
CONFIG_PATH = "config.json"

def load_config(path: str = CONFIG_PATH) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# ---------------- Utils ------------------
def parse_api_time(time_str: str) -> int:
    """æŠŠ m.weibo.cn è¿”å›çš„æ—¶é—´å­—ç¬¦ä¸²è½¬æˆç§’çº§æ—¶é—´æˆ³"""
    try:
        now = datetime.now()
        s = str(time_str)
        if "åˆšåˆš" in s:
            return int(now.timestamp())
        m = re.search(r"(\d+)\s*åˆ†é’Ÿå‰", s)
        if m:
            return int((now - timedelta(minutes=int(m.group(1)))).timestamp())
        h = re.search(r"(\d+)\s*å°æ—¶å‰", s)
        if h:
            return int((now - timedelta(hours=int(h.group(1)))).timestamp())
        y = re.search(r"æ˜¨å¤©\s*(\d{1,2}):(\d{1,2})", s)
        if y:
            hour, minute = int(y.group(1)), int(y.group(2))
            yest = now - timedelta(days=1)
            return int(yest.replace(hour=hour, minute=minute, second=0, microsecond=0).timestamp())

        # å¸¸è§æ ¼å¼å…œåº•
        for fmt in ("%a %b %d %H:%M:%S %z %Y", "%Y-%m-%d %H:%M:%S"):
            try:
                return int(datetime.strptime(s, fmt).timestamp())
            except Exception:
                pass
        return int(now.timestamp())
    except Exception:
        return int(time.time())

def get_delay(delay_cfg) -> float:
    if isinstance(delay_cfg, (list, tuple)) and len(delay_cfg) == 2:
        return random.uniform(delay_cfg[0], delay_cfg[1])
    try:
        return float(delay_cfg)
    except Exception:
        return 2.0

def ensure_db(cfg):
    if Path("db_bootstrap.py").exists():
        try:
            logger.info("æ£€æµ‹åˆ° db_bootstrap.pyï¼Œæ‰§è¡Œæ— æŸå»ºè¡¨ (--mode init)")
            env = os.environ.copy()
            # å¼ºåˆ¶å­è¿›ç¨‹ç”¨ UTF-8 è¾“å‡ºï¼Œé¿å… Windows æ§åˆ¶å° GBK è¯¯è§£ç 
            env["PYTHONIOENCODING"] = "utf-8"
            subprocess.run(
                [sys.executable, "db_bootstrap.py", "--mode", "init"],
                check=False,
                capture_output=True,   # å¦‚éœ€çœ‹åˆ°å­è¿›ç¨‹åŸæ ·è¾“å‡ºï¼Œå¯æ”¹ä¸º False
                text=True,
                encoding="utf-8",      # å…³é”®ï¼šæŒ‰ UTF-8 è§£ç  stdout/stderr
                errors="replace",      # æˆ– "ignore"ï¼šé‡åˆ°ä¸å¯è§£ç å­—ç¬¦ä¸æŠ¥é”™
                env=env,
            )
        except Exception as e:
            logger.warning("æ‰§è¡Œ db_bootstrap.py å¤±è´¥ï¼ˆå¿½ç•¥ç»§ç»­ï¼‰ï¼š%s", e)

# ---------------- DB ---------------------
class DB:
    def __init__(self, mysql_cfg: Dict[str, Any]):
        cfg = mysql_cfg.copy()
        cfg.setdefault("charset", "utf8mb4")
        cfg["port"] = int(cfg.get("port", 3306))
        self.conn = pymysql.connect(**cfg)

    def close(self):
        try:
            self.conn.close()
        except Exception:
            pass

    def ensure_keyword_id(self, keyword: str) -> Optional[int]:
        with self.conn.cursor() as cur:
            cur.execute("SELECT id FROM keywords WHERE keyword=%s", (keyword,))
            row = cur.fetchone()
            if row:
                return int(row[0])
            # æ’å…¥å†å–
            cur.execute("INSERT INTO keywords (keyword) VALUES (%s)", (keyword,))
            kid = int(cur.lastrowid)
        self.conn.commit()
        return kid

    def save_weibo(self, item: Dict[str, Any], keyword_id: int) -> bool:
        sql = """
        INSERT INTO weibo_data (
            id, keyword_id, text, pics, timestamp, source,
            user_name, reposts_count, comments_count, attitudes_count
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        ON DUPLICATE KEY UPDATE
            text=VALUES(text),
            pics=VALUES(pics),
            timestamp=VALUES(timestamp),
            source=VALUES(source),
            user_name=VALUES(user_name),
            reposts_count=VALUES(reposts_count),
            comments_count=VALUES(comments_count),
            attitudes_count=VALUES(attitudes_count),
            updated_at=CURRENT_TIMESTAMP
        """
        try:
            with self.conn.cursor() as cur:
                cur.execute(sql, (
                    item["id"],
                    keyword_id,
                    item.get("text", ""),
                    item.get("pics", ""),
                    int(item.get("timestamp", int(time.time()))),
                    item.get("source", "æ–°æµªå¾®åšç§»åŠ¨ç«¯"),
                    item.get("user_name", ""),
                    int(item.get("reposts_count", 0)),
                    int(item.get("comments_count", 0)),
                    int(item.get("attitudes_count", 0)),
                ))
            self.conn.commit()
            return True
        except Exception as e:
            logger.error("ä¿å­˜å¾®åšå¤±è´¥ id=%sï¼š%s", item.get("id"), e)
            self.conn.rollback()
            return False

    def save_comments(self, comments: List[Dict[str, Any]]) -> int:
        if not comments:
            return 0
        sql = """
        INSERT INTO comments (id, weibo_id, user, content, timestamp)
        VALUES (%s, %s, %s, %s, %s)
        ON DUPLICATE KEY UPDATE
            user=VALUES(user),
            content=VALUES(content),
            timestamp=VALUES(timestamp),
            updated_at=CURRENT_TIMESTAMP
        """
        n = 0
        try:
            with self.conn.cursor() as cur:
                for c in comments:
                    try:
                        cur.execute(sql, (
                            c["id"],
                            c["weibo_id"],
                            c.get("user", ""),
                            c.get("content", ""),
                            int(c.get("timestamp", int(time.time()))),
                        ))
                        n += 1
                    except Exception as e:
                        logger.debug("å¿½ç•¥å•æ¡è¯„è®ºä¿å­˜é”™è¯¯ id=%sï¼š%s", c.get("id"), e)
            self.conn.commit()
            return n
        except Exception as e:
            logger.error("æ‰¹é‡ä¿å­˜è¯„è®ºå¤±è´¥ï¼š%s", e)
            self.conn.rollback()
            return n

# --------------- Crawler -----------------
class WeiboMobileCrawler:
    BASE = "https://m.weibo.cn"

    def __init__(self, config_path: str = CONFIG_PATH):
        self.cfg = load_config(config_path)
        self.session = requests.Session()
        self._setup_session()
        self.db = DB(self.cfg["mysql_config"])

        cs = self.cfg.get("crawler_settings", {})
        self.delay_cfg = cs.get("request_delay", [10, 15])
        self.timeout = int(cs.get("timeout", 20))

        self.crawl_comments = bool(self.cfg.get("crawl_comments", False))
        self.max_comment_pages = int(self.cfg.get("max_comment_pages", 1))

    def _setup_session(self):
        headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Referer": "https://m.weibo.cn/",
            "X-Requested-With": "XMLHttpRequest",
            "Connection": "keep-alive",
        }
        headers.update(self.cfg.get("headers", {}))
        self.session.headers.update(headers)

        proxies = self.cfg.get("proxies") or {}
        if proxies:
            self.session.proxies.update(proxies)

    # ---------- API calls ----------
    def _search_page(self, keyword: str, page: int) -> List[Dict[str, Any]]:
        """è¯·æ±‚æœç´¢é¡µï¼Œè¿”å›æ ‡å‡†åŒ–çš„å¾®åšæ¡ç›®åˆ—è¡¨"""
        url = f"{self.BASE}/api/container/getIndex"
        params = {
            "containerid": f"100103type=1&q={keyword}",
            "page_type": "searchall",
            "page": page,
        }
        r = self.session.get(url, params=params, timeout=self.timeout)
        if r.status_code != 200:
            logger.warning("æœç´¢æ¥å£çŠ¶æ€ç å¼‚å¸¸ %sï¼š%s", r.status_code, r.text[:200])
            return []

        data = r.json()
        ok = data.get("ok")
        if ok != 1:
            logger.warning("æœç´¢æ¥å£è¿”å›é”™è¯¯ ok=%s msg=%s", ok, data.get("msg"))
            return []

        cards = (data.get("data") or {}).get("cards") or []
        items: List[Dict[str, Any]] = []
        for card in cards:
            if card.get("card_type") != 9:
                continue
            mblog = card.get("mblog") or {}
            wid = mblog.get("id")
            if not wid:
                continue

            # æ–‡æœ¬å» HTML
            text = re.sub(r"<[^>]+>", "", mblog.get("text", ""))

            # å›¾ç‰‡
            pics: List[str] = []
            pic_infos = mblog.get("pic_infos") or {}
            for _, info in (pic_infos or {}).items():
                u = info.get("url")
                if u:
                    pics.append(u)
            if not pics and mblog.get("pic_ids"):
                for pid in mblog["pic_ids"]:
                    pics.append(f"https://wx1.sinaimg.cn/large/{pid}.jpg")

            # æ—¶é—´ / ç”¨æˆ·
            ts = parse_api_time(mblog.get("created_at", ""))
            user_name = (mblog.get("user") or {}).get("screen_name", "")

            items.append({
                "id": str(wid),
                "text": text,
                "pics": ",".join(pics) if pics else "",
                "timestamp": ts,
                "source": "æ–°æµªå¾®åšç§»åŠ¨ç«¯",
                "user_name": user_name,
                "reposts_count": int(mblog.get("reposts_count", 0)),
                "comments_count": int(mblog.get("comments_count", 0)),
                "attitudes_count": int(mblog.get("attitudes_count", 0)),
            })
        return items

    def _comments_once(self, weibo_id: str, max_id: Optional[int] = None) -> Dict[str, Any]:
        """è¯·æ±‚ä¸€æ¬¡è¯„è®º APIï¼ˆçƒ­è¯„ï¼‰ï¼Œè¿”å› JSON"""
        url = f"{self.BASE}/comments/hotflow"
        params = {
            "id": weibo_id,
            "mid": weibo_id,
            "max_id_type": 0,
        }
        if max_id:
            params["max_id"] = max_id

        r = self.session.get(url, params=params, timeout=self.timeout)
        if r.status_code != 200:
            return {"ok": 0, "msg": f"HTTP {r.status_code}"}
        try:
            return r.json()
        except Exception:
            return {"ok": 0, "msg": "invalid json"}

    def _crawl_comments(self, weibo_id: str) -> List[Dict[str, Any]]:
        if not self.crawl_comments:
            return []
        max_id = None
        page = 0
        results: List[Dict[str, Any]] = []

        while page < self.max_comment_pages:
            data = self._comments_once(weibo_id, max_id=max_id)
            if data.get("ok") != 1:
                break
            rows = (data.get("data") or {}).get("data") or []
            for it in rows:
                cid = str(it.get("id", ""))
                if not cid:
                    continue
                content = it.get("text") or it.get("text_raw") or ""
                content = re.sub(r"<[^>]+>", "", content)
                ts = parse_api_time(it.get("created_at", "")) if it.get("created_at") else int(time.time())
                results.append({
                    "id": cid,
                    "weibo_id": weibo_id,
                    "user": (it.get("user") or {}).get("screen_name", ""),
                    "content": content.strip(),
                    "timestamp": ts,
                })
            page += 1
            max_id = (data.get("data") or {}).get("max_id")
            if not max_id:
                break
            time.sleep(get_delay(self.delay_cfg) / 2.0)
        return results

    # ---------- Main flow ----------
    def crawl_keywords(self) -> bool:
        try:
            keywords: List[str] = list(self.cfg.get("keywords") or [])
            start_page = int(self.cfg.get("startPage", 1))
            max_page = int(self.cfg.get("maxPage", 1))
        except Exception:
            logger.error("é…ç½®å­—æ®µç¼ºå¤±æˆ–æ ¼å¼é”™è¯¯ï¼ˆkeywords/startPage/maxPageï¼‰")
            return False

        logger.info("è¯„è®ºæŠ“å–ï¼š%sï¼Œæœ€å¤šé¡µæ•°ï¼š%s", self.crawl_comments, self.max_comment_pages)
        total_weibo = 0
        total_comments = 0

        for kw in keywords:
            logger.info("å¼€å§‹çˆ¬å–å…³é”®è¯ï¼š%s", kw)
            kid = self.db.ensure_keyword_id(kw)
            if not kid:
                logger.error("æ— æ³•è·å–/åˆ›å»ºå…³é”®å­—IDï¼š%sï¼Œè·³è¿‡è¯¥å…³é”®è¯", kw)
                continue

            for page in range(start_page, max_page + 1):
                logger.info("å…³é”®è¯ %s â€”â€” ç¬¬ %d é¡µ", kw, page)
                items = self._search_page(kw, page)
                if not items:
                    logger.info("å…³é”®è¯ %s â€”â€” ç¬¬ %d é¡µæ— æ•°æ®ï¼Œåœæ­¢ç¿»é¡µ", kw, page)
                    break

                saved = 0
                for it in items:
                    if self.db.save_weibo(it, kid):
                        saved += 1
                        # è¯„è®º
                        if self.crawl_comments:
                            cmts = self._crawl_comments(it["id"])
                            total_comments += self.db.save_comments(cmts)

                total_weibo += saved
                logger.info("å…³é”®è¯ %s â€”â€” ç¬¬ %d é¡µä¿å­˜ %d æ¡å¾®åš", kw, page, saved)

                delay = get_delay(self.delay_cfg)
                logger.info("ç­‰å¾… %.2f ç§’åç»§ç»­", delay)
                time.sleep(delay)

            logger.info("å…³é”®è¯ %s çˆ¬å–ç»“æŸ", kw)

        logger.info("å…¨éƒ¨å…³é”®è¯å®Œæˆï¼šå¾®åš %d æ¡ï¼Œè¯„è®º %d æ¡", total_weibo, total_comments)
        return True

# ---------------- Entrypoint ----------------
def main():
    print("ğŸš€ å¾®åšç§»åŠ¨ç«¯çˆ¬è™«å¯åŠ¨ä¸­...")
    print("=" * 50)
    # å¯é€‰ï¼šç¡®ä¿è¡¨ç»“æ„
    try:
        ensure_db(load_config().get("mysql_config", {}))
    except Exception:
        pass

    # ç®€å•çš„æ•°æ®åº“è¿é€šæ€§æç¤º
    try:
        db = DB(load_config()["mysql_config"])
        db.close()
        print("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
    except Exception as e:
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼š", e)
        sys.exit(1)

    try:
        crawler = WeiboMobileCrawler()
        ok = crawler.crawl_keywords()
        print("âœ… çˆ¬å–å®Œæˆï¼" if ok else "âŒ çˆ¬å–å¤±è´¥")
        sys.exit(0 if ok else 2)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        logger.exception("è¿è¡Œå¼‚å¸¸ï¼š%s", e)
        sys.exit(1)

if __name__ == "__main__":
    main()
