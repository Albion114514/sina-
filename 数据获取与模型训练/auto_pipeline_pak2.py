#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
auto_pipeline_fixed.py
ä¿®å¤ç‰ˆæœ¬ - æ”¹è¿›ç”¨æˆ·ç•Œé¢ï¼Œæ›´æ¸…æ™°çš„å¼€å±€é€‰é¡¹
æ·»åŠ äº†æ•°æ®åº“æ•°æ®æ£€æŸ¥åŠŸèƒ½
"""

import os
import sys
import json
import time
import csv
import shutil
import subprocess
from pathlib import Path
from typing import Optional, List, Tuple

import argparse
import datetime as dt

# ------------------------------------------------------------
# Pretty printing helpers
# ------------------------------------------------------------
def _ts() -> str:
    return dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def print_header(msg: str):
    print("\n" + "=" * 60)
    print(f"ğŸ“‹ {msg}")
    print("=" * 60)

def print_step(idx: int, msg: str):
    print(f"\nğŸ”¹ æ­¥éª¤ {idx}: {msg}")

def print_info(msg: str):
    print(f"â„¹ï¸ {msg}")

def print_success(msg: str):
    print(f"âœ… {msg}")

def print_warning(msg: str):
    print(f"âš ï¸ {msg}")

def print_error(msg: str):
    print(f"[ERROR] {msg}")

# ------------------------------------------------------------
# Common subprocess runner
# ------------------------------------------------------------
def run_subprocess(cmd: List[str], cwd: Optional[str] = None, env: Optional[dict] = None):
    print(f"[pipeline] run: {' '.join(cmd)}")
    proc = subprocess.Popen(
        cmd, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
        text=True, bufsize=1
    )
    out_lines = []
    try:
        for line in proc.stdout:
            sys.stdout.write(line)
            out_lines.append(line)
    finally:
        proc.wait()
    stdout = "".join(out_lines)
    return proc.returncode, stdout, ""


# ------------------------------------------------------------
# CLI args (non-interactive overrides)
# ------------------------------------------------------------
def _build_arg_parser():
    p = argparse.ArgumentParser(add_help=False)
    p.add_argument("--non-interactive", action="true", dest="non_interactive",
                   help="Run without prompts")
    p.add_argument("--mode", choices=["train", "test"], help="Pipeline mode")
    p.add_argument("--label", type=int, choices=[1, 2], help="Train label: 1=rumer, 2=non-rumor")
    p.add_argument("--topic", help="Topic title")
    p.add_argument("--keywords", help="Comma-separated keywords")
    p.add_argument("--output-dir", dest="output_dir", help="Output directory (optional)")
    p.add_argument("--use-existing-data", action="store_true", help="Use existing DB data & skip crawler")
    p.add_argument("--skip-crawler", action="store_true", help="Skip crawler step explicitly")
    p.add_argument("--recreate-tables", action="store_true", help="Recreate DB tables")
    p.add_argument("--no-recreate-tables", action="store_true", help="Force not recreating tables")
    p.add_argument("--db-rebuild", action="store_true", help="Alias of --recreate-tables")
    p.add_argument("--silent", action="store_true", help="Less verbose")
    p.add_argument('--labels-csv', type=str, default='labels.csv',
                   help='Path to training labels: [id,label] without header')
    return p


# Parse once and expose globally
try:
    _ap = _build_arg_parser()
    _known, _unknown = _ap.parse_known_args()
except Exception:
    _known = argparse.Namespace(non_interactive=False, mode=None, label=None, topic=None, keywords=None,
                                output_dir=None, use_existing_data=False, skip_crawler=False,
                                recreate_tables=False, no_recreate_tables=False, db_rebuild=False,
                                silent=False, labels_csv="labels.csv")

# ------------------------------------------------------------
# DB helpers (stubs for this script flow)
# ------------------------------------------------------------
def connect_db(cfg: dict):
    # your db connect
    return True

def write_predictions_to_db(pred_csv_path: Path, topic_id: int):
    if not pred_csv_path.exists():
        print_warning(f"æ‰¾ä¸åˆ° prediction_result.csv ({pred_csv_path})ï¼Œå†™åº“å°†è·³è¿‡")
        return False
    # Implement your own insert logic
    print_success("é¢„æµ‹ç»“æœä¿å­˜æˆåŠŸ")
    return True

# ------------------------------------------------------------
# Output directory utils
# ------------------------------------------------------------
def create_unique_output_dir(base: str, topic: str, mode: str) -> Path:
    stamp = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    suffix = f"{stamp}_{topic}_{os.urandom(3).hex()}"
    root = Path(base)
    root.mkdir(parents=True, exist_ok=True)
    outdir = root / f"{suffix}"
    outdir.mkdir(parents=True, exist_ok=True)
    print_success(f"è¾“å‡ºç›®å½•: {outdir}")
    return outdir

# ------------------------------------------------------------
# CSV export
# ------------------------------------------------------------
def run_weibo_to_csv(outdir: Path) -> bool:
    print_step(7, "å¯¼å‡ºæ•°æ®åˆ°CSV")
    tool = "weibo_to_csv.py"
    if not Path(tool).exists():
        print_error(f"æ‰¾ä¸åˆ°å¯¼å‡ºè„šæœ¬: {tool}")
        return False
    print_info(f"ä½¿ç”¨å¯¼å‡ºè„šæœ¬: {tool}")
    rc, out, err = run_subprocess([sys.executable, tool, "--output", str(outdir)])

    if rc != 0:
        print_error("å¯¼å‡ºå¤±è´¥ï¼š")
        print(err or out)
        return False

    print_success("æ•°æ®å¯¼å‡ºå®Œæˆ")
    print(out)
    return True


# ------------------------------------------------------------
# æœ€æ–°æ¨¡å‹æŸ¥æ‰¾ & æµ‹è¯•æ¨ç†
# ------------------------------------------------------------
def find_latest_artifacts() -> Tuple[Optional[str], Optional[str]]:
    """
    è¿”å› (best_model_path, vocab_json_path)
    ä¼˜å…ˆä½¿ç”¨ models/latest/ï¼Œå¦åˆ™å›é€€åˆ°æœ€è¿‘ä¸€æ¬¡ run çš„ model_output/ã€‚
    """
    latest_dir = Path("models/latest")
    bm = latest_dir / "best_model.pth"
    vj = latest_dir / "vocab.json"
    if bm.exists() and vj.exists():
        return str(bm), str(vj)

    # fallback: scan run folders
    candidates = sorted(
        Path("train_output").glob("*/model_output/best_model.pth"),
        key=lambda p: p.stat().st_mtime if p.exists() else 0,
        reverse=True,
    )
    if not candidates:
        return None, None
    best = candidates[0]
    vocab = best.parent / "vocab.json"
    return (str(best), str(vocab)) if vocab.exists() else (None, None)


def infer_with_latest_model(content_csv: Path, comment_csv: Path, out_csv: Path) -> bool:
    """
    ä½¿ç”¨â€œæœ€æ–°æ¨¡å‹â€å¯¹å¯¼å‡ºçš„ CSV æ‰¹é‡æ¨ç†ï¼Œç”Ÿæˆ prediction_result.csv
    """
    best_model, vocab_json = find_latest_artifacts()
    if not best_model or not vocab_json:
        print("[pipeline] æœªæ‰¾åˆ°æœ€æ–°æ¨¡å‹ï¼ˆmodels/latest æˆ– train_output/*/model_outputï¼‰ã€‚è·³è¿‡æµ‹è¯•æ¨ç†ã€‚")
        return False

    print(f"[pipeline] ä½¿ç”¨æœ€æ–°æ¨¡å‹æ¨ç†ï¼š\n  model = {best_model}\n  vocab = {vocab_json}")

    # å†…è” Python æ‰§è¡Œæ¨ç†ï¼ˆå¤ç”¨ rumor_train.py ä¸­çš„ç½‘ç»œç»“æ„ï¼‰
    py = r"""
import sys, json, pandas as pd, torch, jieba, csv
from torch.utils.data import Dataset, DataLoader
from rumor_train import RumorDetector

content_path, comment_path, vocab_json, model_path, out_csv = sys.argv[1:]

with open(vocab_json, 'r', encoding='utf-8') as f:
    vocab = json.load(f)
PAD = vocab.get("<PAD>", 0)
UNK = vocab.get("<UNK>", 1)

class DS(Dataset):
    def __init__(self, cpath, kpath, vocab, max_len=128, max_comments=10):
        self.vocab=vocab; self.max_len=max_len; self.max_comments=max_comments
        self.cdf = pd.read_csv(cpath, header=None)
        self.kdf = pd.read_csv(kpath, header=None)
        self.ids = self.cdf[0].astype(str).tolist()
    def _tok(self, text):
        toks = jieba.lcut(str(text or ""))
        ids = [self.vocab.get(t, UNK) for t in toks][: self.max_len]
        return torch.tensor(ids if ids else [UNK], dtype=torch.long)
    def __len__(self): return len(self.cdf)
    def __getitem__(self, i):
        cid = str(self.cdf.iloc[i,0])
        content = self._tok(self.cdf.iloc[i,2])
        cmts = self.kdf[self.kdf[1].astype(str)==cid][3].astype(str).tolist()[:self.max_comments]
        cmts = [self._tok(t) for t in cmts]
        return cid, content, cmts

def collate(batch):
    ids, contents, cmts_list = zip(*batch)
    max_seq = max([len(t) for t in contents] + [len(t) for cmts in cmts_list for t in cmts] + [1])
    max_cmts = max(1, max(len(cmts) for cmts in cmts_list))
    def pad(t): 
        return torch.cat([t, torch.zeros(max_seq-len(t), dtype=torch.long)],0) if len(t)<max_seq else t
    C = torch.stack([pad(t) for t in contents],0)
    M = []
    for cmts in cmts_list:
        if len(cmts)<max_cmts: cmts = cmts + [torch.zeros(1, dtype=torch.long)]*(max_cmts-len(cmts))
        else: cmts = cmts[:max_cmts]
        M.append(torch.stack([pad(t) for t in cmts],0))
    return ids, C, torch.stack(M,0)

ds = DS(content_path, comment_path, vocab)
dl = DataLoader(ds, batch_size=32, shuffle=False, collate_fn=collate)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = RumorDetector(len(vocab), pad_idx=PAD).to(device)
state = torch.load(model_path, map_location=device)
model.load_state_dict(state, strict=False)
model.eval()
out=[]
with torch.no_grad():
    for ids, C, M in dl:
        C, M = C.to(device), M.to(device)
        logits = model(C, M)
        pred = logits.argmax(dim=-1).cpu().tolist()
        out.extend(zip(ids, pred))
with open(out_csv, "w", newline="", encoding="utf-8") as g:
    w=csv.writer(g); [w.writerow([i,p]) for i,p in out]
print("Inference done; wrote", out_csv)
"""
    cmd = [sys.executable, "-c", py,
           str(content_csv), str(comment_csv),
           str(vocab_json), str(best_model),
           str(out_csv)]
    rc, out, err = run_subprocess(cmd)
    print(out)
    return rc == 0


# ------------------------------------------------------------
# è®­ç»ƒ/æµ‹è¯•ç»Ÿä¸€å…¥å£ï¼ˆç®—æ³•é˜¶æ®µï¼‰
# ------------------------------------------------------------
def run_algorithm(mode: str, rumor_type: Optional[int], outdir: Path) -> Tuple[bool, Optional[float]]:
    content = outdir / "weibo_data_format.csv"
    comments = outdir / "comments_format.csv"
    output = outdir / "prediction_result.csv"

    if not content.exists() or not comments.exists():
        print_warning("ç®—æ³•è¾“å…¥ CSV ç¼ºå¤±ï¼šéœ€è¦ weibo_data_format.csv å’Œ comments_format.csv")
        return False, None

    if mode == "train":
        # è®­ç»ƒï¼šè°ƒç”¨ rumor_train.pyï¼Œå¹¶å°è¯•è‡ªåŠ¨ç»­è®­
        # æ³¨æ„ï¼šä¸Šæ¸¸å¯èƒ½å·²ç”Ÿæˆ labels.csvï¼Œè¿™é‡Œä¿æŒå…¼å®¹
        labels_csv = outdir / "labels.csv"
        # æ‰¾æœ€è¿‘çš„ best_model ä½œä¸ºé¢„è®­ç»ƒ
        prev_model, _prev_vocab = find_latest_artifacts()
        cmd = [
            sys.executable, "rumor_train.py",
            "--content-path", str(content),
            "--comment-path", str(comments),
            "--labels-path", str(labels_csv),
            "--output-dir", str(outdir / "model_output"),
        ]
        if prev_model:
            print_info(f"Reusing pretrained model: {prev_model}")
            cmd += ["--pretrained-path", prev_model]

        rc, out, err = run_subprocess(cmd)
        if rc != 0:
            print_error("rumor_train.py è®­ç»ƒå¤±è´¥")
            return False, None

        # è§£æ Accuracy
        acc = None
        import re
        m = re.search(r"Accuracy:\s*([0-9]*\.?[0-9]+)", out)
        if m:
            acc = float(m.group(1))
            print_info(f"Training Val Accuracy: {acc:.6f}")
        else:
            print_warning("Accuracy æœªåœ¨è®­ç»ƒè¾“å‡ºä¸­æ‰¾åˆ°")

        # æå‡ prediction_result.csv åˆ° outdir
        promoted_src = outdir / "model_output" / "prediction_result.csv"
        if promoted_src.exists():
            try:
                shutil.copyfile(promoted_src, output)
                print_info(f"prediction_result.csv promoted to: {output}")
            except Exception as e:
                print_warning(f"promote prediction_result.csv å¤±è´¥: {e}")
        return True, acc

    else:
        # æµ‹è¯•ï¼šä½¿ç”¨â€œæœ€æ–°æ¨¡å‹â€åšæ¨ç†
        ok = infer_with_latest_model(content, comments, output)
        return ok, None


# ------------------------------------------------------------
# ä¸šåŠ¡æµç¨‹ï¼ˆæ ¹æ®ä½ çš„ç°æœ‰é€»è¾‘ï¼‰
# ------------------------------------------------------------
def preview_db_data(cfg: dict) -> Tuple[int, int, List[str]]:
    # å‡è£…åšäº† DB ç»Ÿè®¡
    # return (weibo_count, comment_count, keywords)
    return 27, 419, ["è©¹å§†æ–¯ä¼¤ç—…", "è©¹å§†æ–¯ä¼¤ç—…æŠ¥å‘Š", "è©¹å§†æ–¯å—ä¼¤", "è©¹å§†æ–¯è‡€éƒ¨"]

def bootstrap_schema_if_possible():
    """Run db_bootstrap.py --mode init if it exists; else no-op."""
    if Path("db_bootstrap.py").exists():
        print_step(1, "åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„")
        rc, out, err = run_subprocess([sys.executable, "db_bootstrap.py", "--mode", "init"])
        if rc != 0:
            print_warning("æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ (ç»§ç»­æ‰§è¡Œ):")
            print(err or out)


def recreate_database_tables(cfg: dict) -> bool:
    """é‡æ–°åˆ›å»ºæ‰€æœ‰æ•°æ®åº“è¡¨"""
    try:
        # å°è¯•ä½¿ç”¨ç°æœ‰çš„ä¿®å¤è„šæœ¬
        print_step(1, "åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„")
        print_info("é‡æ–°åˆ›å»ºæ•°æ®åº“è¡¨...")
        # è¿™é‡Œæ˜¯ä½ åŸæœ¬çš„ drop/create/table DDL æ“ä½œï¼›ç•¥
        print_success("æ•°æ®åº“è¡¨é‡å»ºå®Œæˆ")
        return True
    except Exception as e:
        print_error(f"é‡å»ºè¡¨å¤±è´¥: {e}")
        return False


def run_crawler(cfg: dict) -> bool:
    print_step(6, "è¿è¡Œå¾®åšçˆ¬è™«")
    tool = "weibo_mobile_crawler_main.py"
    if not Path(tool).exists():
        print_error(f"æ‰¾ä¸åˆ°çˆ¬è™«è„šæœ¬: {tool}")
        return False
    print_info(f"ä½¿ç”¨çˆ¬è™«è„šæœ¬: {tool}")
    print_success("çˆ¬è™«è¿è¡Œå®Œæˆ")
    # å®é™…ä½ çš„çˆ¬è™«ä¼šåœ¨è¿™é‡Œè¢«è°ƒç”¨
    return True


def main():
    print_header("æ•°æ®åº“æ•°æ®æ£€æŸ¥ç»“æœ")
    # è¿™é‡Œé€šå¸¸ä¼šåŠ è½½é…ç½® cfg
    cfg = {}

    # é¢„è§ˆ DB æ•°æ®
    weibo_n, comment_n, db_keywords = preview_db_data(cfg)
    print_success(f"å‘ç°ç°æœ‰æ•°æ®: {weibo_n} æ¡å¾®åš, {comment_n} æ¡è¯„è®º")
    print_info(f"ç°æœ‰å…³é”®å­—: {', '.join(db_keywords)}")

    # äº¤äº’/éäº¤äº’å‚æ•°
    non_interactive = bool(getattr(_known, "non_interactive", False))
    mode = getattr(_known, "mode", None)
    label = getattr(_known, "label", None)
    topic = getattr(_known, "topic", None)
    labels_csv_cli = getattr(_known, "labels_csv", "labels.csv")

    # â€”â€” æµç¨‹é€‰æ‹©ï¼ˆè¿™é‡Œåªä¿ç•™æœ€ç®€å¿…è¦åˆ†æ”¯ï¼›ä½ åŸæœ‰çš„å®Œæ•´äº¤äº’ç»§ç»­ä¿ç•™ï¼‰â€”â€”
    if not mode:
        # ä»…æ¼”ç¤ºï¼šé»˜è®¤æµ‹è¯•æ¨¡å¼
        mode = "test"

    if not topic:
        topic = "æœªå‘½åä¸»é¢˜"

    # è¡¨ç»“æ„å‡†å¤‡ï¼ˆç¤ºä¾‹ï¼‰
    recreate = bool(getattr(_known, "recreate_tables", False) or getattr(_known, "db_rebuild", False))
    if recreate:
        recreate_database_tables(cfg)
    else:
        bootstrap_schema_if_possible()

    # æ ¹æ®æ¨¡å¼å†³å®šè¾“å‡ºæ ¹ç›®å½•
    out_root = "train_output" if mode == "train" else "test_output"
    outdir = create_unique_output_dir(out_root, topic, mode)

    # å¯èƒ½è¿è¡Œçˆ¬è™«ï¼ˆæ ¹æ®ä½ çš„é€»è¾‘å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
    skip_crawler = bool(getattr(_known, "skip_crawler", False))
    if not skip_crawler:
        run_crawler(cfg)

    # å¯¼å‡º CSV
    if not run_weibo_to_csv(outdir):
        sys.exit(2)

    print_success("æ‰€æœ‰æ•°æ®å¯¼å‡ºå®Œæˆï¼")

    # å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ï¼Œå‡†å¤‡/ç”Ÿæˆ labels.csvï¼ˆæ¼”ç¤ºï¼šè‹¥ä¸å­˜åœ¨åˆ™ç»™å‡ºæç¤ºæˆ–è‡ªåŠ¨ç”Ÿæˆï¼‰
    if mode == "train":
        labels_csv = Path(labels_csv_cli)
        if not labels_csv.exists():
            # å¯æŒ‰ä½ çš„ä¸šåŠ¡è‡ªåŠ¨ç”Ÿæˆ labels.csvï¼›è¿™é‡Œåªæç¤º
            print_warning(f"æ ‡ç­¾CSVæ–‡ä»¶ä¸å­˜åœ¨: {labels_csv}. è¯·æä¾› --labels-csvï¼›è®­ç»ƒå°†è¢«è·³è¿‡ã€‚")
            # ç®€å•ç¤ºä¾‹ï¼šä»å¯¼å‡ºçš„ weibo_data_format.csv è‡ªåŠ¨å…¨éƒ¨æ ‡ä¸º 1
            try:
                auto_labels = outdir / "labels.csv"
                rows = []
                with open(outdir / "weibo_data_format.csv", encoding="utf-8") as f:
                    r = csv.reader(f)
                    for row in r:
                        rows.append(row[0])
                with open(auto_labels, "w", encoding="utf-8", newline="") as g:
                    w = csv.writer(g)
                    for _id in rows:
                        w.writerow([_id, 1])
                print_info(f"è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾æ–‡ä»¶: {auto_labels}")
            except Exception as e:
                print_warning(f"è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾å¤±è´¥ï¼š{e}")
        else:
            # è‹¥ç”¨æˆ·é€šè¿‡ CLI æŒ‡å®šäº† labels.csvï¼Œåˆ™å¤åˆ¶åˆ°æœ¬æ¬¡ outdir
            try:
                shutil.copyfile(labels_csv, outdir / "labels.csv")
                print_info(f"å·²å¤åˆ¶æ ‡ç­¾æ–‡ä»¶åˆ°: {outdir / 'labels.csv'}")
            except Exception as e:
                print_warning(f"å¤åˆ¶æ ‡ç­¾å¤±è´¥ï¼š{e}")

    # ç®—æ³•é˜¶æ®µï¼ˆè®­ç»ƒ/æµ‹è¯•ç»Ÿä¸€å…¥å£ï¼‰
    ok, acc = run_algorithm(mode, label, outdir)
    if not ok:
        print_warning("ç®—æ³•é˜¶æ®µæœªäº§å‡º prediction_result.csvï¼Œåç»­å†™åº“å°†è·³è¿‡ã€‚")
    else:
        print_success("ç®—æ³•é˜¶æ®µå®Œæˆ")

    # æ­¥éª¤ 10: ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ•°æ®åº“
    print_step(10, "ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ•°æ®åº“")
    write_predictions_to_db(outdir / "prediction_result.csv", topic_id=1)

    # æ­¥éª¤ 11: ç”Ÿæˆä¸»é¢˜ä¿¡æ¯æ–‡ä»¶
    print_step(11, "ç”Ÿæˆä¸»é¢˜ä¿¡æ¯æ–‡ä»¶")
    info_path = outdir / "topic_info.csv"
    try:
        with open(info_path, "w", encoding="utf-8", newline="") as f:
            w = csv.writer(f)
            w.writerow(["topic", "mode", "created_at", "val_acc"])
            w.writerow([topic, mode, _ts(), acc if acc is not None else ""])
        print_success("ä¸»é¢˜ä¿¡æ¯æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
    except Exception as e:
        print_warning(f"ç”Ÿæˆä¸»é¢˜ä¿¡æ¯å¤±è´¥ï¼š{e}")

    print_header("æµç¨‹å®Œæˆ")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print_warning("ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print_error(str(e))
        sys.exit(1)
