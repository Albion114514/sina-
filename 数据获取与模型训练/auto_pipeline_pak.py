#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
auto_pipeline_fixed.py
ä¿®å¤ç‰ˆæœ¬ - æ”¹è¿›ç”¨æˆ·ç•Œé¢ï¼Œæ›´æ¸…æ™°çš„å¼€å±€é€‰é¡¹
æ·»åŠ äº†æ•°æ®åº“æ•°æ®æ£€æŸ¥åŠŸèƒ½
"""

import os
import sys
import json
import time
import csv
import shutil
import subprocess
from pathlib import Path
# ---- NON_INTERACTIVE_CLI_PATCH: argparse flags ----
import argparse
import re
from pathlib import Path
def _build_arg_parser():
    p = argparse.ArgumentParser(add_help=False)
    p.add_argument("--non-interactive", action="store_true", help="Run without prompts")
    p.add_argument("--mode", choices=["train","test"], help="Pipeline mode")
    p.add_argument("--label", type=int, choices=[1,2], help="Train label: 1=rumer, 2=non-rumor")
    p.add_argument("--topic", help="Topic title")
    p.add_argument("--keywords", help="Comma-separated keywords")
    p.add_argument("--output-dir", dest="output_dir", help="Output directory (optional)")
    p.add_argument("--use-existing-data", action="store_true", help="Use existing DB data & skip crawler")
    p.add_argument("--skip-crawler", action="store_true", help="Skip crawler step explicitly")
    p.add_argument("--recreate-tables", action="store_true", help="Recreate DB tables")
    p.add_argument("--no-recreate-tables", action="store_true", help="Force not recreating tables")
    p.add_argument("--db-rebuild", action="store_true", help="Alias of --recreate-tables")
    p.add_argument("--silent", action="store_true", help="Less verbose")
    p.add_argument('--labels-csv', type=str, default='labels.csv',
                   help='Path to training labels: [id,label] without header')
    return p

# Parse once and expose globally
try:
    _ap = _build_arg_parser()
    _known, _unknown = _ap.parse_known_args()
except Exception:
    _known = argparse.Namespace(non_interactive=False, mode=None, label=None, topic=None, keywords=None,
                                output_dir=None, use_existing_data=False, skip_crawler=False,
                                recreate_tables=False, no_recreate_tables=False, db_rebuild=False, silent=False)
ARGS = _known
# -----------------------------------------------

from uuid import uuid4
from datetime import datetime
from typing import Optional, List, Tuple

# ---------- Constants ----------
TRAIN_OUTPUT_DIR = "train_output"
TEST_OUTPUT_DIR = "test_output"

WEIBO_TO_CSV_SCRIPT_CANDIDATES = ["weibo_to_csv.py"]
CRAWLER_SCRIPT_CANDIDATES = [
    "weibo_mobile_crawler_main.py",
    "weibo_crawler_2_1_main.py",
    "weibo_crawler_main.py",
]
ALGO_SCRIPT_CANDIDATES = ["rumor_detect.py"]

CONFIG_PATH = "config.json"


# ---------- Utilities ----------

def ensure_module_or_none(modname: str):
    """Try to import a module; if missing, return None."""
    try:
        return __import__(modname)
    except Exception:
        return None


settings = ensure_module_or_none("settings")


def load_config(path: str = CONFIG_PATH) -> dict:
    """Load config.json; if not present, raise clear error."""
    if settings and hasattr(settings, "load_config"):
        return settings.load_config(path)
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def build_sqlalchemy_uri(cfg: dict) -> str:
    """Return a SQLAlchemy URI built from cfg['mysql_config'] (fallback if settings absent)."""
    if settings and hasattr(settings, "build_sqlalchemy_uri"):
        return settings.build_sqlalchemy_uri(cfg)
    mc = cfg["mysql_config"].copy()
    user = mc.get("user", "root")
    pwd = mc.get("password", "")
    host = mc.get("host", "localhost")
    port = int(mc.get("port", 3306))
    db = mc.get("database", "")
    return f"mysql+pymysql://{user}:{pwd}@{host}:{port}/{db}?charset=utf8mb4"


def run_subprocess(cmd: List[str], cwd: Optional[str] = None) -> Tuple[int, str, str]:
    """Run a subprocess and return (returncode, stdout, stderr)."""
    try:
        p = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore',
            cwd=cwd
        )
        return p.returncode, p.stdout, p.stderr
    except UnicodeDecodeError:
        # å¦‚æœutf-8å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
        p = subprocess.run(
            cmd,
            capture_output=True,
            cwd=cwd
        )
        stdout = p.stdout.decode('utf-8', errors='ignore')
        stderr = p.stderr.decode('utf-8', errors='ignore')
        return p.returncode, stdout, stderr


def run_cmd(cmd, cwd=None, env=None):
    print(f"[pipeline] run: {' '.join(cmd)}")

    # è®¾ç½®ç¯å¢ƒå˜é‡
    if env is None:
        env = os.environ.copy()
    env['PYTHONIOENCODING'] = 'utf-8'
    env['PYTHONUTF8'] = '1'

    try:
        # ä½¿ç”¨subprocess.runå¹¶æ•è·è¾“å‡º
        result = subprocess.run(
            cmd,
            cwd=cwd,
            env=env,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace'  # æ›¿æ¢æ— æ³•è§£ç çš„å­—ç¬¦
        )

        # æ‰“å°æ ‡å‡†è¾“å‡º
        if result.stdout:
            print(result.stdout)

        # æ‰“å°æ ‡å‡†é”™è¯¯
        if result.stderr:
            print(f"[STDERR] {result.stderr}")

        return result.returncode, result.stdout + result.stderr

    except Exception as e:
        print(f"[ERROR] è¿è¡Œå‘½ä»¤æ—¶å‡ºé”™: {e}")
        return 1, str(e)

def parse_accuracy(stdout_text: str):
    m = re.search(r'Accuracy:\s*([0-9]*\.?[0-9]+)', stdout_text)
    return float(m.group(1)) if m else None
def pick_existing(paths: List[str]) -> Optional[str]:
    """Return the first existing path from candidates."""
    for p in paths:
        if Path(p).exists():
            return p
    return None


def slugify(text: str, max_len: int = 32) -> str:
    """Safe slug from text for filesystem names."""
    safe = "".join(ch if ch.isalnum() else "_" for ch in text.strip())
    if not safe:
        safe = "topic"
    return safe[:max_len].strip("_")


def print_header(title: str):
    """æ‰“å°ç¾è§‚çš„æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f"ğŸ“‹ {title}")
    print("=" * 60)


def print_step(step_num: int, description: str):
    """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
    print(f"\nğŸ”¹ æ­¥éª¤ {step_num}: {description}")


def print_success(message: str):
    """æ‰“å°æˆåŠŸä¿¡æ¯"""
    print(f"âœ… {message}")


def print_warning(message: str):
    """æ‰“å°è­¦å‘Šä¿¡æ¯"""
    print(f"âš ï¸ {message}")


def print_error(message: str):
    """æ‰“å°é”™è¯¯ä¿¡æ¯"""
    print(f"âŒ {message}")


def print_info(message: str):
    """æ‰“å°ä¸€èˆ¬ä¿¡æ¯"""
    print(f"â„¹ï¸ {message}")


# ---------- DB Helpers (pymysql) ----------

def get_db_conn(cfg: dict):
    import pymysql
    mc = cfg["mysql_config"].copy()
    mc.setdefault("charset", "utf8mb4")
    mc["port"] = int(mc.get("port", 3306))
    return pymysql.connect(**mc)


def bootstrap_schema_if_possible():
    """Run db_bootstrap.py --mode init if it exists; else no-op."""
    if Path("db_bootstrap.py").exists():
        print_step(1, "åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„")
        rc, out, err = run_subprocess([sys.executable, "db_bootstrap.py", "--mode", "init"])
        if rc != 0:
            print_warning("æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ (ç»§ç»­æ‰§è¡Œ):")
            print(err or out)


def recreate_database_tables(cfg: dict) -> bool:
    """é‡æ–°åˆ›å»ºæ‰€æœ‰æ•°æ®åº“è¡¨"""
    try:
        # å°è¯•ä½¿ç”¨ç°æœ‰çš„ä¿®å¤è„šæœ¬
        if Path("db_setup_fixed.py").exists():
            print_info("ä½¿ç”¨ db_setup_fixed.py é‡å»ºè¡¨...")
            rc, out, err = run_subprocess([sys.executable, "db_setup_fixed.py"])
            if rc == 0:
                print_success("è¡¨é‡å»ºæˆåŠŸ")
                return True
            else:
                print_error("è¡¨é‡å»ºå¤±è´¥")
                return False
        elif Path("db_setup2_0.py").exists():
            print_info("ä½¿ç”¨ db_setup2_0.py é‡å»ºè¡¨...")
            rc, out, err = run_subprocess([sys.executable, "db_setup2_0.py"])
            if rc == 0:
                print_success("è¡¨é‡å»ºæˆåŠŸ")
                return True
            else:
                print_error("è¡¨é‡å»ºå¤±è´¥")
                return False
        else:
            print_warning("æœªæ‰¾åˆ°æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬ï¼Œä½¿ç”¨å†…ç½®æ–¹æ³•é‡å»º...")
            return recreate_tables_manual(cfg)
    except Exception as e:
        print_error(f"è¡¨é‡å»ºå¤±è´¥: {e}")
        return False


def recreate_tables_manual(cfg: dict) -> bool:
    """æ‰‹åŠ¨é‡å»ºè¡¨ç»“æ„"""
    try:
        conn = get_db_conn(cfg)
        cursor = conn.cursor()

        print_info("åˆ é™¤å¤–é”®çº¦æŸ...")
        cursor.execute("""
            SELECT CONSTRAINT_NAME, TABLE_NAME 
            FROM information_schema.KEY_COLUMN_USAGE 
            WHERE TABLE_SCHEMA = %s 
            AND REFERENCED_TABLE_NAME IS NOT NULL
        """, (cfg['mysql_config']['database'],))

        foreign_keys = cursor.fetchall()
        for fk_name, table_name in foreign_keys:
            try:
                cursor.execute(f"ALTER TABLE {table_name} DROP FOREIGN KEY {fk_name}")
                print_success(f"åˆ é™¤å¤–é”®: {table_name}.{fk_name}")
            except Exception as e:
                print_warning(f"åˆ é™¤å¤–é”®å¤±è´¥ {table_name}.{fk_name}: {e}")

        # åˆ é™¤è¡¨ï¼ˆæŒ‰ä¾èµ–é¡ºåºï¼‰
        tables_to_drop = [
            'topic_keywords', 'prediction_results', 'comments',
            'weibo_data', 'crawl_log', 'keywords', 'topics'
        ]

        for table in tables_to_drop:
            try:
                cursor.execute(f"DROP TABLE IF EXISTS {table}")
                print_success(f"åˆ é™¤è¡¨: {table}")
            except Exception as e:
                print_warning(f"åˆ é™¤è¡¨å¤±è´¥ {table}: {e}")

        print_info("åˆ›å»ºæ–°è¡¨...")
        # åˆ›å»ºå…³é”®å­—è¡¨
        cursor.execute("""
            CREATE TABLE keywords (
                id INT AUTO_INCREMENT PRIMARY KEY,
                keyword VARCHAR(255) NOT NULL UNIQUE,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                INDEX idx_keyword (keyword)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
        """)
        print_success("åˆ›å»ºå…³é”®å­—è¡¨")

        # åˆ›å»ºå¾®åšæ•°æ®è¡¨
        cursor.execute("""
            CREATE TABLE weibo_data (
                id VARCHAR(50) PRIMARY KEY,
                keyword_id INT NOT NULL,
                text TEXT NOT NULL,
                pics TEXT,
                timestamp BIGINT NOT NULL,
                source VARCHAR(100) DEFAULT 'æ–°æµªå¾®åšç§»åŠ¨ç«¯',
                user_name VARCHAR(255) DEFAULT '',
                reposts_count INT DEFAULT 0,
                comments_count INT DEFAULT 0,
                attitudes_count INT DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                INDEX idx_timestamp (timestamp),
                INDEX idx_keyword_id (keyword_id),
                FOREIGN KEY (keyword_id) REFERENCES keywords(id)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
        """)
        print_success("åˆ›å»ºå¾®åšæ•°æ®è¡¨")

        # åœ¨ recreate_tables_manual() é‡Œã€åˆ›å»º weibo_data ä¹‹åï¼Œç»§ç»­åŠ ä¸Šï¼š

        # comments
        cursor.execute("""
            CREATE TABLE comments (
                id VARCHAR(50) PRIMARY KEY,
                weibo_id VARCHAR(50) NOT NULL,
                user VARCHAR(100),
                content TEXT NOT NULL,
                timestamp BIGINT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                INDEX idx_weibo_id (weibo_id),
                FOREIGN KEY (weibo_id) REFERENCES weibo_data(id)
                    ON DELETE CASCADE ON UPDATE RESTRICT
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
        """)

        # crawl_log
        cursor.execute("""
            CREATE TABLE crawl_log (
                id INT AUTO_INCREMENT PRIMARY KEY,
                keyword VARCHAR(255) NOT NULL,
                page_num INT NOT NULL,
                crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                success_count INT DEFAULT 0,
                error_message TEXT,
                INDEX idx_keyword (keyword)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
        """)

        # topics
        cursor.execute("""
            CREATE TABLE topics (
                id INT AUTO_INCREMENT PRIMARY KEY,
                title VARCHAR(255) NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                mode ENUM('train','test') NOT NULL,
                rumor_type TINYINT,
                UNIQUE KEY uk_title (title)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
        """)

        # topic_keywords
        cursor.execute("""
            CREATE TABLE topic_keywords (
                id INT AUTO_INCREMENT PRIMARY KEY,
                topic_id INT NOT NULL,
                keyword_id INT NOT NULL,
                UNIQUE KEY uk_topic_keyword (topic_id, keyword_id),
                FOREIGN KEY (topic_id) REFERENCES topics(id) ON DELETE CASCADE,
                FOREIGN KEY (keyword_id) REFERENCES keywords(id)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
        """)

        # prediction_results
        cursor.execute("""
            CREATE TABLE prediction_results (
                id INT AUTO_INCREMENT PRIMARY KEY,
                topic_id INT NOT NULL,
                prediction TEXT NOT NULL,
                accuracy FLOAT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE KEY uk_topic (topic_id),
                FOREIGN KEY (topic_id) REFERENCES topics(id) ON DELETE CASCADE
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
        """)

        # æ’å…¥é»˜è®¤å…³é”®å­—
        keywords = cfg.get('keywords', [])
        for keyword in keywords:
            cursor.execute("INSERT IGNORE INTO keywords (keyword) VALUES (%s)", (keyword,))
        print_success(f"æ’å…¥ {len(keywords)} ä¸ªå…³é”®å­—")

        conn.commit()
        conn.close()
        return True

    except Exception as e:
        print_error(f"æ‰‹åŠ¨é‡å»ºè¡¨å¤±è´¥: {e}")
        return False


def check_existing_data(cfg: dict) -> dict:
    """æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰æ•°æ®ï¼Œè¿”å›æ•°æ®çŠ¶æ€"""
    try:
        conn = get_db_conn(cfg)
        cursor = conn.cursor()

        # æ£€æŸ¥å¾®åšæ•°æ®
        cursor.execute("SELECT COUNT(*) FROM weibo_data")
        weibo_count = cursor.fetchone()[0]

        # æ£€æŸ¥è¯„è®ºæ•°æ®
        cursor.execute("SELECT COUNT(*) FROM comments")
        comment_count = cursor.fetchone()[0]

        # æ£€æŸ¥å…³é”®å­—
        cursor.execute("SELECT keyword FROM keywords")
        existing_keywords = [row[0] for row in cursor.fetchall()]

        # è·å–è¯„è®ºé¢„è§ˆ
        comments_preview = []
        if comment_count > 0:
            cursor.execute("SELECT content FROM comments LIMIT 10")
            comments_preview = [row[0] for row in cursor.fetchall()]

        conn.close()

        return {
            'has_data': weibo_count > 0 or comment_count > 0,
            'weibo_count': weibo_count,
            'comment_count': comment_count,
            'existing_keywords': existing_keywords,
            'comments_preview': comments_preview
        }
    except Exception as e:
        print_warning(f"æ£€æŸ¥ç°æœ‰æ•°æ®æ—¶å‡ºé”™: {e}")
        return {
            'has_data': False,
            'weibo_count': 0,
            'comment_count': 0,
            'existing_keywords': [],
            'comments_preview': []
        }


def display_data_preview(data_status: dict):
    """æ˜¾ç¤ºç°æœ‰æ•°æ®çš„é¢„è§ˆ"""
    print_header("æ•°æ®åº“æ•°æ®æ£€æŸ¥ç»“æœ")

    if not data_status['has_data']:
        print_info("æ•°æ®åº“ä¸­æš‚æ— æ•°æ®ï¼Œå°†æ‰§è¡Œå®Œæ•´æµç¨‹")
        return

    print_success(f"å‘ç°ç°æœ‰æ•°æ®: {data_status['weibo_count']} æ¡å¾®åš, {data_status['comment_count']} æ¡è¯„è®º")

    # æ˜¾ç¤ºå…³é”®å­—
    if data_status['existing_keywords']:
        print_info(f"ç°æœ‰å…³é”®å­—: {', '.join(data_status['existing_keywords'])}")

    # è¯¢é—®æ˜¯å¦æ˜¾ç¤ºè¯„è®ºé¢„è§ˆ
    if data_status['comments_preview']:
        show_preview = input("æ˜¯å¦æ˜¾ç¤ºè¯„è®ºçš„å‰åè¡Œï¼Ÿ(y/n): ").strip().lower()
        if show_preview == 'y':
            print("\nğŸ“ è¯„è®ºå‰åè¡Œé¢„è§ˆ:")
            for i, comment in enumerate(data_status['comments_preview'], 1):
                print(f"  {i}. {comment[:100]}{'...' if len(comment) > 100 else ''}")
            print()


def get_existing_data_choice(cfg: dict, data_status: dict) -> dict:
    """è·å–ç”¨æˆ·å¯¹ç°æœ‰æ•°æ®çš„å¤„ç†é€‰æ‹©"""
    # Non-interactive short-circuit
    try:
        if ARGS.non_interactive:
            use_existing = bool(ARGS.use_existing_data) and data_status.get('has_data', False)
            skip_crawler = bool(ARGS.skip_crawler or ARGS.use_existing_data)
            return {'use_existing_data': use_existing, 'skip_crawler': skip_crawler}
    except NameError:
        pass
    if not data_status['has_data']:
        return {'use_existing_data': False}

    print_header("ç°æœ‰æ•°æ®å¤„ç†é€‰é¡¹")

    # æ˜¾ç¤ºé…ç½®å…³é”®å­—å’Œç°æœ‰å…³é”®å­—çš„æ¯”è¾ƒ
    config_keywords = set(cfg.get('keywords', []))
    existing_keywords = set(data_status['existing_keywords'])

    print("å…³é”®å­—å¯¹æ¯”:")
    print(f"  ğŸ“‹ é…ç½®æ–‡ä»¶å…³é”®å­—: {', '.join(config_keywords)}")
    print(f"  ğŸ—ƒï¸  æ•°æ®åº“ç°æœ‰å…³é”®å­—: {', '.join(existing_keywords)}")

    if config_keywords == existing_keywords:
        print_success("å…³é”®å­—åŒ¹é…ï¼å¯ä»¥ä½¿ç”¨ç°æœ‰æ•°æ®è¿›è¡Œè®­ç»ƒ/æµ‹è¯•")
        use_existing = input("æ˜¯å¦ç›´æ¥ä½¿ç”¨ç°æœ‰æ•°æ®è¿›è¡Œè®­ç»ƒ/æµ‹è¯•ï¼Ÿ(y/n): ").strip().lower()
        if use_existing == 'y':
            return {
                'use_existing_data': True,
                'skip_crawler': True,
                'keywords_match': True
            }

    print_info("å…³é”®å­—ä¸åŒ¹é…æˆ–é€‰æ‹©é‡æ–°çˆ¬å–ï¼Œå°†æ‰§è¡Œå®Œæ•´æµç¨‹")
    return {'use_existing_data': False}


# ---------- Core Steps ----------

def get_user_choices(cfg: dict, use_existing_data: bool = False):
    """è·å–ç”¨æˆ·çš„æ‰€æœ‰é€‰æ‹©"""
    # Non-interactive short-circuit
    try:
        if ARGS.non_interactive:
            mode = ARGS.mode or cfg.get('mode', 'train')
            topic_title = ARGS.topic or cfg.get('topic_title') or 'weibo_topic'
            rumor_type = None
            if mode == 'train':
                rumor_type = ARGS.label if ARGS.label in (1,2) else cfg.get('rumor_type', 1)
            # keywords
            if ARGS.keywords:
                kw = [k.strip() for k in ARGS.keywords.split(",") if k.strip()]
            else:
                kw = cfg.get('keywords', [])
            # recreate choice
            recreate_choice = 'n'
            if ARGS.recreate_tables or ARGS.db_rebuild:
                recreate_choice = 'y'
            if ARGS.no_recreate_tables:
                recreate_choice = 'n'
            return {
                'recreate_tables': (recreate_choice == 'y'),
                'mode': mode,
                'topic_title': topic_title,
                'rumor_type': rumor_type,
                'keywords': kw,
            }
    except NameError:
        pass
    print_header("å¾®åšè°£è¨€æ£€æµ‹è‡ªåŠ¨åŒ–ç³»ç»Ÿ")
    print("æ¬¢è¿ä½¿ç”¨è‡ªåŠ¨åŒ–æµç¨‹ï¼è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œé…ç½®ï¼š")

    # æ­¥éª¤1: æ•°æ®åº“è¡¨é‡å»ºé€‰æ‹©ï¼ˆåªæœ‰åœ¨ä¸ä½¿ç”¨ç°æœ‰æ•°æ®æ—¶æ‰è¯¢é—®ï¼‰
    if not use_existing_data:
        print_step(1, "æ•°æ®åº“è¡¨é…ç½®")
        print("å½“å‰æ•°æ®åº“çŠ¶æ€ï¼š")
        print("  - å¦‚æœé¦–æ¬¡è¿è¡Œæˆ–éœ€è¦é‡ç½®æ•°æ®ï¼Œè¯·é€‰æ‹©é‡å»º")
        print("  - å¦‚æœå·²æœ‰æ•°æ®å¹¶å¸Œæœ›ä¿ç•™ï¼Œè¯·é€‰æ‹©ä½¿ç”¨ç°æœ‰è¡¨")

        recreate_choice = None
        while recreate_choice not in ("y", "n", "Y", "N"):
            recreate_choice = input("æ˜¯å¦é‡æ–°åˆ›å»ºæ•°æ®åº“è¡¨ï¼Ÿ(y/n): ").strip().lower()
    else:
        recreate_choice = 'n'

    # æ­¥éª¤2: å…³é”®å­—ç¡®è®¤
    print_step(2, "å…³é”®å­—é…ç½®")
    current_keywords = cfg.get('keywords', [])
    print(f"å½“å‰é…ç½®çš„å…³é”®å­—: {current_keywords}")
    print("è¿™äº›å…³é”®å­—å°†ç”¨äºå¾®åšæœç´¢")

    if use_existing_data:
        keyword_choice = 'y'
        print_info("ä½¿ç”¨ç°æœ‰æ•°æ®ï¼Œè‡ªåŠ¨ç¡®è®¤å…³é”®å­—")
    else:
        keyword_choice = None
        while keyword_choice not in ("y", "n", "Y", "N"):
            keyword_choice = input("ç¡®è®¤ä½¿ç”¨è¿™äº›å…³é”®å­—ï¼Ÿ(y/n): ").strip().lower()

    if keyword_choice != "y":
        print("è¯·ä¿®æ”¹ config.json æ–‡ä»¶ä¸­çš„ keywords é…ç½®åé‡æ–°è¿è¡Œç¨‹åºã€‚")
        sys.exit(1)

    # æ­¥éª¤3: æ¨¡å¼é€‰æ‹©
    print_step(3, "è¿è¡Œæ¨¡å¼é€‰æ‹©")
    print("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š")
    print("  - è®­ç»ƒæ¨¡å¼ (1): ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œéœ€è¦æŒ‡å®šå†…å®¹ç±»å‹")
    print("  - æµ‹è¯•æ¨¡å¼ (2): ç”¨äºæµ‹è¯•æ¨¡å‹ï¼Œä¸éœ€è¦æŒ‡å®šå†…å®¹ç±»å‹")

    mode = None
    while mode not in ("1", "2"):
        mode = input("è¯·é€‰æ‹©æ¨¡å¼ (1:è®­ç»ƒ, 2:æµ‹è¯•): ").strip()
    mode = "train" if mode == "1" else "test"

    # æ­¥éª¤4: å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ï¼Œé€‰æ‹©å†…å®¹ç±»å‹
    rumor_type = None
    if mode == "train":
        print_step(4, "è®­ç»ƒå†…å®¹ç±»å‹é€‰æ‹©")
        print("è¯·é€‰æ‹©è®­ç»ƒæ•°æ®çš„ç±»å‹ï¼š")
        print("  - è°£è¨€ (1): æ ‡è®°ä¸ºè°£è¨€çš„æ•°æ®")
        print("  - éè°£è¨€ (2): æ ‡è®°ä¸ºéè°£è¨€çš„æ•°æ®")

        rt = None
        while rt not in ("1", "2"):
            rt = input("è¯·é€‰æ‹©å†…å®¹ç±»å‹ (1:è°£è¨€, 2:éè°£è¨€): ").strip()
        rumor_type = int(rt)

    # æ­¥éª¤5: è¾“å…¥å¤§æ ‡é¢˜
    print_step(5, "ä¸»é¢˜å‘½å")
    print("è¯·è¾“å…¥æœ¬æ¬¡ä»»åŠ¡çš„å¤§æ ‡é¢˜ï¼š")
    print("  - è¿™å°†ç”¨äºåˆ›å»ºè¾“å‡ºç›®å½•å’Œæ•°æ®åº“è®°å½•")
    print("  - å»ºè®®ä½¿ç”¨æœ‰æ„ä¹‰çš„åç§°ï¼Œå¦‚'çº¢å†›åŸå¤§æ·è°£è¨€æ£€æµ‹'")

    topic_title = ""
    while not topic_title.strip():
        topic_title = input("è¯·è¾“å…¥å¤§æ ‡é¢˜: ").strip()
        if not topic_title.strip():
            print_warning("å¤§æ ‡é¢˜ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")

    return {
        'recreate_tables': recreate_choice == 'y',
        'keywords_confirmed': keyword_choice == 'y',
        'mode': mode,
        'rumor_type': rumor_type,
        'topic_title': topic_title,
        'keywords': current_keywords
    }


def ensure_keywords(cur, keywords: List[str]) -> List[int]:
    """Ensure keywords exist; return their IDs in order."""
    ids: List[int] = []
    for k in keywords:
        cur.execute("SELECT id FROM keywords WHERE keyword=%s", (k,))
        row = cur.fetchone()
        if row:
            ids.append(int(row[0]))
        else:
            cur.execute("INSERT INTO keywords (keyword) VALUES (%s)", (k,))
            ids.append(int(cur.lastrowid))
    return ids


def save_topic_and_link_keywords(conn, title: str, mode: str, rumor_type: Optional[int], kw_list: List[str]) -> int:
    """Insert or update topic, link to keywords in topic_keywords; return topic_id."""
    with conn.cursor() as cur:
        # upsert topic
        cur.execute(
            """
            INSERT INTO topics (title, mode, rumor_type)
            VALUES (%s, %s, %s)
            ON DUPLICATE KEY UPDATE mode=VALUES(mode), rumor_type=VALUES(rumor_type)
            """,
            (title, mode, rumor_type),
        )
        if cur.lastrowid:
            topic_id = int(cur.lastrowid)
        else:
            cur.execute("SELECT id FROM topics WHERE title=%s", (title,))
            topic_id = int(cur.fetchone()[0])

        # ensure keywords and link
        kw_ids = ensure_keywords(cur, kw_list)
        for kid in kw_ids:
            cur.execute(
                """
                INSERT IGNORE INTO topic_keywords (topic_id, keyword_id)
                VALUES (%s, %s)
                """,
                (topic_id, kid),
            )
    conn.commit()
    return topic_id


def run_crawler():
    """Run the crawler script (pick the first that exists)."""
    crawler = pick_existing(CRAWLER_SCRIPT_CANDIDATES)
    if not crawler:
        print_warning("æœªæ‰¾åˆ°çˆ¬è™«è„šæœ¬ï¼Œè¯·å°†çˆ¬è™«è„šæœ¬å‘½åä¸ºä»¥ä¸‹ä¹‹ä¸€å¹¶æ”¾åœ¨å½“å‰ç›®å½•ï¼š")
        for c in CRAWLER_SCRIPT_CANDIDATES:
            print("  -", c)
        return False

    print_step(6, "è¿è¡Œå¾®åšçˆ¬è™«")
    print_info(f"ä½¿ç”¨çˆ¬è™«è„šæœ¬: {crawler}")
    rc, out, err = run_subprocess([sys.executable, crawler])

    if rc != 0:
        print_error("çˆ¬è™«è¿è¡Œå¤±è´¥ï¼š")
        print(err or out)
        return False

    print_success("çˆ¬è™«è¿è¡Œå®Œæˆ")
    print(out)
    return True


def create_unique_output_dir(mode: str, topic_title: str) -> Path:
    base = TRAIN_OUTPUT_DIR if mode == "train" else TEST_OUTPUT_DIR
    Path(base).mkdir(exist_ok=True)
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    slug = slugify(topic_title, max_len=24)
    unique = f"{stamp}_{slug}_{uuid4().hex[:6]}"
    outdir = Path(base) / unique
    outdir.mkdir(parents=True, exist_ok=False)
    print_success(f"è¾“å‡ºç›®å½•: {outdir}")
    return outdir


def run_weibo_to_csv(outdir: Path) -> bool:
    """Run weibo_to_csv export into outdir; expects script exists."""
    tool = pick_existing(WEIBO_TO_CSV_SCRIPT_CANDIDATES)
    if not tool:
        print_warning("æœªæ‰¾åˆ° weibo_to_csv.pyï¼Œè¯·å°†è„šæœ¬æ”¾åœ¨å½“å‰ç›®å½•")
        return False

    print_step(7, "å¯¼å‡ºæ•°æ®åˆ°CSV")
    print_info(f"ä½¿ç”¨å¯¼å‡ºè„šæœ¬: {tool}")
    rc, out, err = run_subprocess([sys.executable, tool, "--output", str(outdir)])

    if rc != 0:
        print_error("å¯¼å‡ºå¤±è´¥ï¼š")
        print(err or out)
        return False

    print_success("æ•°æ®å¯¼å‡ºå®Œæˆ")
    print(out)
    return True


# auto_pipeline.py ä¸­çš„ run_algorithm() å»ºè®®æ›¿æ¢ä¸ºï¼š
'''def run_algorithm(mode: str, rumor_type: Optional[int], outdir: Path) -> Tuple[bool, Optional[float]]:
    content = outdir / "weibo_data_format.csv"
    comments = outdir / "comments_format.csv"
    output = outdir / "prediction_result.csv"

    if not content.exists() or not comments.exists():
        print_warning("ç®—æ³•è¾“å…¥ CSV ç¼ºå¤±ï¼šéœ€è¦ weibo_data_format.csv å’Œ comments_format.csv")
        return False, None

    if mode == "train":
        if rumor_type not in (1, 2):
            print_error("è®­ç»ƒæ¨¡å¼éœ€è¦ --labelï¼ˆ1=è°£è¨€, 2=éè°£è¨€ï¼‰")
            return False, None
        cmd = [
            sys.executable, "rumor_train.py",
            "--content", str(content),
            "--comments", str(comments),
            "--label", str(rumor_type),
            "--model_out", "models/rumor_lstm.pt",
            "--output", str(output),  # å…¼å®¹åŸæœ‰â€œç”Ÿæˆé¢„æµ‹CSVâ€çš„è¡Œä¸º
        ]
    else:
        cmd = [
            sys.executable, "rumor_test.py",
            "--content", str(content),
            "--comments", str(comments),
            "--output", str(output),
            "--model_in", "models/rumor_lstm.pt",
        ]

    print_step(8, "è¿è¡Œè°£è¨€æ£€æµ‹ç®—æ³•")
    print_info("è¿è¡Œå‘½ä»¤: " + " ".join(cmd))
    rc, out, err = run_subprocess(cmd)

    if rc != 0:
        print_error("ç®—æ³•è¿è¡Œå¤±è´¥ï¼š")
        print(err or out)
        return False, None

    acc = None
    for line in (out or "").splitlines():
        if "Accuracy" in line:
            try:
                acc = float(line.split(":")[-1].strip())
            except Exception:
                pass

    print_success("ç®—æ³•è¿è¡Œå®Œæˆ")
    print(out)
    return True, acc
'''


def save_prediction(conn, topic_id: int, outdir: Path, accuracy: Optional[float]) -> bool:
    """Read prediction_result.csv content and store into prediction_results table (1:1 topic)."""
    pred_path = outdir / "prediction_result.csv"
    if not pred_path.exists():
        print_warning("æ‰¾ä¸åˆ° prediction_result.csvï¼Œå†™åº“å°†è·³è¿‡")
        return False

    content = pred_path.read_text(encoding="utf-8", errors="ignore")

    with conn.cursor() as cur:
        cur.execute(
            """
            INSERT INTO prediction_results (topic_id, prediction, accuracy)
            VALUES (%s, %s, %s)
            ON DUPLICATE KEY UPDATE
                prediction=VALUES(prediction),
                accuracy=VALUES(accuracy)
            """,
            (topic_id, content, accuracy),
        )
    return True


def write_topic_info_csv(conn, topic_id: int, outdir: Path) -> None:
    """Write topic_info.csv with topic + prediction join into outdir."""
    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT t.title, t.mode, t.rumor_type, p.prediction, p.accuracy, t.created_at
            FROM topics t
            LEFT JOIN prediction_results p ON t.id = p.topic_id
            WHERE t.id = %s
            """,
            (topic_id,),
        )
        row = cur.fetchone()

    csv_path = outdir / "topic_info.csv"
    with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["title", "mode", "rumor_type", "prediction", "accuracy", "created_at"])
        if row:
            writer.writerow(row)


def main():
    # è®¾ç½®UTF-8ç¼–ç ç¯å¢ƒ
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    os.environ['PYTHONUTF8'] = '1'

    # åŠ è½½é…ç½®
    cfg = load_config()

    # æ£€æŸ¥ç°æœ‰æ•°æ®
    data_status = check_existing_data(cfg)
    display_data_preview(data_status)

    # è·å–ç”¨æˆ·å¯¹ç°æœ‰æ•°æ®çš„å¤„ç†é€‰æ‹©
    existing_data_choice = get_existing_data_choice(cfg, data_status)
    use_existing_data = existing_data_choice.get('use_existing_data', False)
    skip_crawler = existing_data_choice.get('skip_crawler', False)

    # è·å–ç”¨æˆ·é€‰æ‹©
    choices = get_user_choices(cfg, use_existing_data)

    # æ•°æ®åº“è¡¨å¤„ç†
    if choices['recreate_tables']:
        print_info("é‡æ–°åˆ›å»ºæ•°æ®åº“è¡¨...")
        if recreate_database_tables(cfg):
            print_success("æ•°æ®åº“è¡¨é‡å»ºå®Œæˆ")
        else:
            print_error("æ•°æ®åº“è¡¨é‡å»ºå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ç°æœ‰è¡¨")
    else:
        print_info("ä½¿ç”¨ç°æœ‰æ•°æ®åº“è¡¨")
        bootstrap_schema_if_possible()

    # æ•°æ®åº“è¿æ¥
    conn = get_db_conn(cfg)
    try:
        # ä¿å­˜ä¸»é¢˜å’Œå…³è”å…³é”®å­—
        print_step(9, "ä¿å­˜ä¸»é¢˜ä¿¡æ¯åˆ°æ•°æ®åº“")
        topic_id = save_topic_and_link_keywords(
            conn,
            choices['topic_title'],
            choices['mode'],
            choices['rumor_type'],
            choices['keywords']
        )
        print_success(f"ä¸»é¢˜ä¿å­˜æˆåŠŸï¼ŒID: {topic_id}")

        # è¿è¡Œçˆ¬è™«ï¼ˆå¦‚æœä¸éœ€è¦è·³è¿‡ï¼‰
        if skip_crawler:
            print_info("è·³è¿‡çˆ¬è™«æ­¥éª¤ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®")
        else:
            if not run_crawler():
                from sys import exit as _exit
                try:
                    _ni = ARGS.non_interactive
                except NameError:
                    _ni = False
                if not _ni:
                    yn = input("çˆ¬è™«è¿è¡Œå¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­åç»­æ­¥éª¤? (y/n): ").strip().lower()
                    if yn != "y":
                        _exit(1)
                else:
                    print_warning("éäº¤äº’æ¨¡å¼ï¼šçˆ¬è™«å¤±è´¥ä½†ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        outdir = None
        try:
            if ARGS.non_interactive and ARGS.output_dir:
                outdir = Path(ARGS.output_dir)
                outdir.mkdir(parents=True, exist_ok=True)
        except NameError:
            pass
        if outdir is None:
            outdir = create_unique_output_dir(choices['mode'], choices['topic_title'])
        export_dir = outdir  # ä¸è¡¥ä¸ä¸­çš„export_dirå¯¹åº”

        # å¯¼å‡ºCSV
        if not run_weibo_to_csv(outdir):
            yn = input("å¯¼å‡ºå¤±è´¥ï¼Œæ˜¯å¦ç»§ç»­ç®—æ³•æ­¥éª¤? (y/n): ").strip().lower()
            if yn != "y":
                sys.exit(1)

        # ------------ æ–°å¢è®­ç»ƒé€»è¾‘ï¼ˆæ›¿ä»£åŸrun_algorithmï¼‰ ------------
        # ç¡®è®¤è®­ç»ƒæ ‡ç­¾æ–‡ä»¶å­˜åœ¨
        labels_csv = ARGS.labels_csv
        # è‹¥ labels.csv ä¸å­˜åœ¨ã€ä¸”æ˜¯è®­ç»ƒæ¨¡å¼å¹¶ä¸”é€‰æ‹©äº† rumor_typeï¼Œåˆ™è‡ªåŠ¨ä» weibo_data_format.csv ç”Ÿæˆ
        if not os.path.exists(labels_csv):
            if choices.get('mode') == 'train' and choices.get('rumor_type') in (1, 2):
                auto_labels_path = os.path.join(export_dir, 'labels.csv')
                try:
                    import csv
                    with open(os.path.join(export_dir, 'weibo_data_format.csv'), newline='', encoding='utf-8') as f, \
                            open(auto_labels_path, 'w', newline='', encoding='utf-8') as g:
                        r = csv.reader(f)
                        w = csv.writer(g)
                        # 1=è°£è¨€â†’æ ‡ç­¾1ï¼›2=éè°£è¨€â†’æ ‡ç­¾0
                        label_value = 1 if choices['rumor_type'] == 1 else 0
                        for row in r:
                            w.writerow([row[0], label_value])
                    labels_csv = auto_labels_path
                    print(f"[pipeline] Auto-generated labels at: {labels_csv}")
                    do_training = True
                except Exception as e:
                    print(f"[WARN] auto-generate labels failed: {e}")
                    do_training = False
            else:
                print(
                    f"[WARN] labels csv not found at {labels_csv}. Please provide --labels-csv; training will be skipped.")
                do_training = False

        # è®­ç»ƒé˜¶æ®µï¼šè°ƒç”¨rumor_train.py
        if do_training:
            model_out_dir = os.path.join(export_dir, 'model_output')
            Path(model_out_dir).mkdir(exist_ok=True, parents=True)

            # æ„å»ºè®­ç»ƒå‘½ä»¤
            content_csv = os.path.join(export_dir, 'weibo_data_format.csv')
            comment_csv = os.path.join(export_dir, 'comments_format.csv')
            # åœ¨ auto_pipeline.py ä¸­æ‰¾åˆ°è®­ç»ƒå‘½ä»¤éƒ¨åˆ†ï¼Œä¿®æ”¹ä¸ºï¼š
            train_cmd = [
                sys.executable, 'rumor_train.py',
                '--content-path', content_csv,
                '--comment-path', comment_csv,
                '--labels-path', labels_csv,
                '--output-dir', model_out_dir,
                '--epochs', '6',
                '--batch-size', '32',
                '--lr', '1e-3',
                '--max-len', '128',
                '--max-comments', '10',
            ]
            rc, out = run_cmd(train_cmd)
            if rc != 0:
                print("[ERROR] rumor_train.py failed.")
                sys.exit(rc)

            # è§£æå‡†ç¡®ç‡
            acc = parse_accuracy(out)
            if acc is not None:
                print(f"[pipeline] Training Val Accuracy: {acc:.6f}")
            else:
                print("[pipeline] Accuracy not found in rumor_train.py output.")

            # æ¨å¹¿é¢„æµ‹ç»“æœæ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
            pred_csv = os.path.join(model_out_dir, 'prediction_result.csv')
            if os.path.exists(pred_csv):
                promoted = os.path.join(export_dir, 'prediction_result.csv')
                try:
                    shutil.copyfile(pred_csv, promoted)
                    print(f"[pipeline] prediction_result.csv promoted to: {promoted}")
                except Exception as e:
                    print(f"[WARN] failed to promote prediction_result.csv: {e}")

        # ------------ ä¿ç•™åŸæœ‰çš„ç»“æœå†™å…¥æ•°æ®åº“é€»è¾‘ ------------
        print_step(10, "ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ•°æ®åº“")
        pred_csv_path = os.path.join(export_dir, 'prediction_result.csv')
        if not os.path.exists(pred_csv_path):
            print_warning(f"æ‰¾ä¸åˆ° prediction_result.csv ({pred_csv_path})ï¼Œå†™åº“å°†è·³è¿‡")
        else:
            if save_prediction(conn, topic_id, outdir, acc):  # ä¼ å…¥è§£æå¾—åˆ°çš„å‡†ç¡®ç‡
                print_success("é¢„æµ‹ç»“æœä¿å­˜æˆåŠŸ")
            else:
                print_warning("é¢„æµ‹ç»“æœä¿å­˜å¤±è´¥")

        # å†™å…¥topic_info.csv
        print_step(11, "ç”Ÿæˆä¸»é¢˜ä¿¡æ¯æ–‡ä»¶")
        write_topic_info_csv(conn, topic_id, outdir)
        print_success("ä¸»é¢˜ä¿¡æ¯æ–‡ä»¶ç”Ÿæˆå®Œæˆ")

        # æ˜¾ç¤ºå®Œæˆä¿¡æ¯
        print_header("æµç¨‹å®Œæˆ")
        # ... ä¿ç•™åŸæœ‰çš„å®Œæˆä¿¡æ¯è¾“å‡º ...

    except Exception as e:
        print_error(f"æµç¨‹æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        conn.close()


if __name__ == "__main__":
    main()