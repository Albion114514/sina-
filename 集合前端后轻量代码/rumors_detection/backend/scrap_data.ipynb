{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59cb75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a63969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def __init__(self):\n",
    "        self.cookie = {\"Cookie\":\"SINAGLOBAL=7973288796724.17.1664160723535; _s_tentry=passport.weibo.com; Apache=5236423547984.983.1760949988989; ULV=1760949989028:1:1:1:5236423547984.983.1760949988989:; XSRF-TOKEN=NMKl19R6mErEsyh3O20FXXFD; SCF=Ak36-mSiYJz2fFofSw7NNYDa9Zd0oqgXgNpbcBeZT-658IYW925SM3m7nG10PtUUwFMXbr2EdBzemCcfB2lG6MA.; SUB=_2A25F8YctDeRhGeNO6VUT9S_EyDWIHXVmjoblrDV8PUNbmtANLU_xkW9NTx_CECBgrfnNUj_JnKqmNe1wEtgfDoF1; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9W5g955MZ9Aq.BJbDC2nE.J05NHD95QfehzNeo-p1he4Ws4Dqcjdi--4iK.4iKnRi--ciKLhiKn4i--RiKyhiKn0; ALF=02_1763542141; WBPSESS=RCFA8dQwx0lf-E-lv3bEF8MQrd_qI519KTlN39KqnyegiXqMGV_Kr3eED6bSio9bX7SUykfOskN_f4y4JSbCfI5xUB2PqnDg5TOC49OEzQz1fBzZeaNo9tdVT-YqbtdRLqyOLWRkV1xHbRyHcpCrag==\"}\n",
    "        self.cookie_mobile = {\"Cookie\":\"SCF=Ak36-mSiYJz2fFofSw7NNYDa9Zd0oqgXgNpbcBeZT-658IYW925SM3m7nG10PtUUwJgxAMrTeZFJ0DwC4s9M2yc.; SUB=_2A25F8YctDeRhGeNO6VUT9S_EyDWIHXVmjoblrDV6PUJbktCOLWjlkW1NTx_CEHvunA42vrdq3iib374i4hPtzK8e; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9W5g955MZ9Aq.BJbDC2nE.J05NHD95QfehzNeo-p1he4Ws4Dqcjdi--4iK.4iKnRi--ciKLhiKn4i--RiKyhiKn0; SSOLoginState=1760950141; ALF=1763542141; _T_WM=f083755324d8f59bd817020ccf769702\"}\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Cache-Control': 'no-cache',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Pragma': 'no-cache',\n",
    "            'Referer': 'https://weibo.com/'\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "        self.session.cookies.update(self.cookie)\n",
    "    def random_delay(self, min_delay=2, max_delay=5):\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "    def rotate_user_agent(self):\n",
    "        user_agents = [\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n",
    "    \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; 360SE)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)\",\n",
    "    \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1\",\n",
    "    \"Mozilla/5.0 (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0b13pre) Gecko/20110307 Firefox/4.0b13pre\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:16.0) Gecko/20100101 Firefox/16.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
    "        ]\n",
    "        return random.choice(user_agents)\n",
    "    def is_anti_crawler(self, response):\n",
    "        \"\"\"检测是否触发了反爬虫\"\"\"\n",
    "        anti_crawler_indicators = [\n",
    "            len(response.text) < 5000,  # 内容过少\n",
    "            '验证' in response.text,     # 包含验证关键词\n",
    "            'passport' in response.url,  # 被重定向到登录页\n",
    "            'security' in response.text, # 安全验证\n",
    "            'access denied' in response.text.lower(),\n",
    "        ]\n",
    "        return any(anti_crawler_indicators)\n",
    "    def convert_to_mobile_url(self, url):\n",
    "        \"\"\"将微博PC链接转换为移动端链接\"\"\"\n",
    "        # 匹配 weibo.com 并替换为 weibo.cn\n",
    "        mobile_url = re.sub(r'https?://weibo\\.com', 'https://weibo.cn', url)\n",
    "        return mobile_url\n",
    "    def keyword_query(self,keyword):\n",
    "        '''通过关键字搜索得到的文章url'''\n",
    "        init_url = 'https://s.weibo.com/weibo'\n",
    "        url = init_url + '/' + keyword\n",
    "        self.session.cookies.update(self.cookie)\n",
    "        res = self.session.get(url,timeout=10)\n",
    "        urls = []\n",
    "        if res.status_code == 200:\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            \n",
    "            from_divs = soup.find_all('div', class_='from')\n",
    "    \n",
    "            for div in from_divs:\n",
    "                a_tag = div.find('a')\n",
    "                if a_tag and a_tag.has_attr('href'):\n",
    "                    href_url = \"https:\" + a_tag['href']\n",
    "                    urls.append(self.convert_to_mobile_url(href_url))\n",
    "        self.random_delay()\n",
    "        self.headers[\"User-Agent\"] = self.rotate_user_agent()\n",
    "        return urls\n",
    "    def content_comment(self,url):\n",
    "        '''获取url对应文章的相关信息'''\n",
    "        print(f\"query:{url}\")\n",
    "        page = 0\n",
    "        self.session.cookies.update(self.cookie_mobile)\n",
    "        comments_count = 0\n",
    "        content = {\n",
    "            'text':'',\n",
    "            'img':[],\n",
    "            'time':''\n",
    "        }\n",
    "        comments = []\n",
    "        while(1):\n",
    "            page += 1\n",
    "            res = self.session.get(url + f\"&page={page}\",timeout=10)\n",
    "            tmp_ct = 0\n",
    "            if res.status_code == 200:\n",
    "                soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                detail_divs = soup.find_all('div', class_='c')\n",
    "                for i, div in enumerate(detail_divs, 1):\n",
    "                    a = div.find('a')\n",
    "                    if not a:\n",
    "                        continue\n",
    "                    name = a.get_text()\n",
    "                    span = div.find('span', class_='ctt')\n",
    "                    if not span:\n",
    "                        continue\n",
    "                    text = span.get_text(strip=True)\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    tmp_ct += 1\n",
    "                    if tmp_ct == 1:\n",
    "                        if content['text'] == '':\n",
    "                            content['text'] = text[1:]\n",
    "                            print(f\"正文内容:{content['text']}\")\n",
    "                            images = div.find_all('img',class_='ib')\n",
    "                            for img in images:\n",
    "                                img_src = img.get('src')\n",
    "                                content['img'].append(img_src)\n",
    "                            content_time = div.find('span',class_='ct').get_text()[:19]\n",
    "                            if '今天' in content_time:\n",
    "                                current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "                                content_time = current_time[:11] + content_time[3:8] + \":00\"\n",
    "                            elif '分钟前' in content_time:\n",
    "                                minite = int(content_time.split('分')[0])\n",
    "                                now = datetime.now()\n",
    "                                content_time = (now - timedelta(minutes=minite)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                            elif '月' in content_time:\n",
    "                                current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "                                if content_time[9] == ':':\n",
    "                                    content_time = current_time[:5] + content_time[:2] + current_time[7] + content_time[3:5] + current_time[10] + content_time[7:12] + current_time[16:]\n",
    "                                elif content_time[7] == ':':\n",
    "                                    content_time = current_time[:5] + '0' + content_time[0] + current_time[7] + '0' + content_time[2] + current_time[10] + content_time[5:10] + current_time[16:]\n",
    "                                elif content_time[1] == '月':\n",
    "                                    content_time = current_time[:5] + '0' + content_time[0] + current_time[7] + content_time[2:4] + current_time[10] + content_time[6:11] + current_time[16:]\n",
    "                                else :\n",
    "                                    content_time = current_time[:5] + content_time[:2] + current_time[7] + '0' + content_time[3] + current_time[10] + content_time[6:11] + current_time[16:]\n",
    "                            content['time'] = content_time\n",
    "\n",
    "                        continue\n",
    "                    comments_count += 1\n",
    "                    is_reply = text[0] == '@'\n",
    "                    rep_time = div.find('span',class_='ct').get_text()[:19]\n",
    "                    tmp_cmt = {\n",
    "                        'text':text,\n",
    "                        'name':name,\n",
    "                        'is_reply':is_reply,\n",
    "                        'reply2':'',\n",
    "                        'time':rep_time,\n",
    "                    }\n",
    "                    if is_reply:\n",
    "                        a_ = span.find('a')\n",
    "                        reply2 = a_.get_text()[1:]\n",
    "                        tmp_cmt['reply2'] = reply2\n",
    "                    comments.append(tmp_cmt)\n",
    "                    if comments_count < 10:\n",
    "                        print(f\"第{comments_count}条微博评论: {text}\")\n",
    "                        print(\"-\" * 50)\n",
    "            else:\n",
    "                print(\"wrong!\")\n",
    "                break\n",
    "            if tmp_ct == 1:\n",
    "                return content,comments\n",
    "    def query(self,keyword,times = 3):\n",
    "        '''外部通过关键词查询以及设置查询的数量限制'''\n",
    "        urls = self.keyword_query(keyword)\n",
    "        contents = []\n",
    "        comments = []\n",
    "        ct = 0\n",
    "        for url in urls:\n",
    "            c1,c2 = self.content_comment(url)\n",
    "            contents.append(c1)\n",
    "            comments.append(c2)\n",
    "            ct += 1\n",
    "            if ct == times:\n",
    "                break\n",
    "        return contents,comments\n",
    "    def downloadimg(self,img_url,path='image.jpg'):\n",
    "        '''下载图片到指定路径'''\n",
    "        response = requests.get(img_url, headers=self.headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"图片下载成功: {path}\")\n",
    "        else:\n",
    "            print(\"下载失败!\")\n",
    "    def hot_search(self):\n",
    "        url = 'https://weibo.com/ajax/side/hotSearch'\n",
    "        self.session.cookies.update(self.cookie)\n",
    "        res = self.session.get(url,timeout=10)\n",
    "        if res.status_code == 200:\n",
    "            data = res.json()\n",
    "            texts = []\n",
    "            nums = []\n",
    "            if 'data' in data and 'realtime' in data['data']:\n",
    "                for item in data['data']['realtime']:\n",
    "                    word = item.get('word', '')\n",
    "                    note = item.get('note', '')\n",
    "                    num = int(item.get('num', ''))\n",
    "                    text = note if note else word\n",
    "                    if text:\n",
    "                        texts.append(text)\n",
    "                        nums.append(num)\n",
    "            \n",
    "            print(f\"获取到 {len(texts)} 条热搜\")\n",
    "            return texts,nums\n",
    "        else:\n",
    "            print(f\"请求失败，状态码: {res.status_code}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02060c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9192bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:https://weibo.cn/5304798547/Jy0w916la?refer_flag=1001030103_\n",
      "正文内容:现存最早的DOTA2游戏记录，诞生在2010/12/9，我当时正忙着StarCraft2，手忙脚乱，不知道在世界另一头有个游戏诞生了，而且会在九年后影响我。网页链接各位，十年前那时候，你们在做什么呢？\n",
      "第1条微博评论: 刚好在11奋战刀1\n",
      "--------------------------------------------------\n",
      "第2条微博评论: 回复@动人的山斐:我还买的光盘6.68c。我接触的第一张图。也是玩了1年多电脑。不知道可以联机。ap。rd 都不知道。以为能虐电脑。我超牛逼 后来登浩方进去玩。被虐得好惨......\n",
      "--------------------------------------------------\n",
      "第3条微博评论: 回复@梦碎温哥华:说出来你可能不信  当时不知道能联机 玩了一年多单机  把各种技能 装备合成背的滚瓜烂熟  直到在hao123最底下看到了11的广告 才打开了我的新世界\n",
      "--------------------------------------------------\n",
      "第4条微博评论: 回复@动人的山斐:不应该是浩方吗\n",
      "--------------------------------------------------\n",
      "第5条微博评论: 回复@田教主說愛要謙卑:不对不对，toc叫十字军的试炼，杠一下\n",
      "--------------------------------------------------\n",
      "第6条微博评论: 回复@假九精灵:整个版本是巫妖王之怒啦\n",
      "--------------------------------------------------\n",
      "第7条微博评论: 地下城与勇士\n",
      "--------------------------------------------------\n",
      "第8条微博评论: 回复@田教主說愛要謙卑:巫妖王不是icc么？\n",
      "--------------------------------------------------\n",
      "第9条微博评论: 回复@三年-鱼:都是好事儿\n",
      "--------------------------------------------------\n",
      "query:https://weibo.cn/1715118170/M8VgOqPs8?refer_flag=1001030103_\n",
      "正文内容:【Do you wonder how a StarCraft2 bot is coded? Come watch me in this live coding session!】网页链接你想知道星际争霸 2 机器人是如何编码的吗？ 来观看我的现场编码课程吧！网路冷眼的微博视频\n",
      "query:https://weibo.cn/1402400261/NCoYn40d0?refer_flag=1001030103_\n",
      "正文内容:【Large Language Models Play StarCraft II: 支持大型语言模型(LLM)玩StarCraft II的纯语言环境，开发了TextStarCraft II文本环境，提出一种多级摘要方法，用于处理原始观察数据、分析游戏信息、提供命令建议和生成战略决策，实验证明LLM智能体能在Harder(Lv5)难度级别下击败内置AI】’Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach - TextStarCraft2,a pure language env which support llms play starcraft2' GitHub: github.com/histmeisah/Large-Language-Models-play-StarCraftII#开源##机器学习##人工智能#\n",
      "第1条微博评论: @我的印象笔记\n",
      "--------------------------------------------------\n",
      "第2条微博评论: @我的印象笔记\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "keyword = 'starcraft2'\n",
    "times = 3\n",
    "contents,comments = scraper.query(keyword,times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61bb31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '现存最早的DOTA2游戏记录，诞生在2010/12/9，我当时正忙着StarCraft2，手忙脚乱，不知道在世界另一头有个游戏诞生了，而且会在九年后影响我。网页链接各位，十年前那时候，你们在做什么呢？',\n",
       " 'img': ['https://wx3.sinaimg.cn/wap180/005N0oqTgy1glk7jwnqnnj30oc1g012y.jpg'],\n",
       " 'time': '2020-12-11 20:53:59'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c39a7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '刚好在11奋战刀1',\n",
       " 'name': '茶壶泡栗子',\n",
       " 'is_reply': False,\n",
       " 'reply2': '',\n",
       " 'time': '2020-12-11 20:54:51'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('weibo_result.html', 'w', encoding='utf-8') as f:\n",
    "#     f.write(res.text)\n",
    "\n",
    "# print(\"结果已保存到 weibo_result.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11df1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import jieba\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c42198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumorDataset(Dataset):\n",
    "    def __init__(self, contents, comments_list, labels, vocab, max_content_len=200, max_comment_len=50, max_comments=100):\n",
    "        self.contents = contents\n",
    "        self.comments_list = comments_list\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_content_len = max_content_len\n",
    "        self.max_comment_len = max_comment_len\n",
    "        self.max_comments = max_comments\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def text_to_sequence(self, text):\n",
    "        \"\"\"将文本转换为序列\"\"\"\n",
    "        words = jieba.lcut(text)\n",
    "        sequence = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
    "        return sequence\n",
    "    \n",
    "    def pad_sequence(self, sequence, max_len):\n",
    "        \"\"\"填充序列\"\"\"\n",
    "        if len(sequence) > max_len:\n",
    "            return sequence[:max_len]\n",
    "        else:\n",
    "            return sequence + [self.vocab['<PAD>']] * (max_len - len(sequence))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        content_text = self.contents[idx]['text']\n",
    "        content_seq = self.text_to_sequence(content_text)\n",
    "        content_padded = self.pad_sequence(content_seq, self.max_content_len)\n",
    "        \n",
    "        comments = self.comments_list[idx]\n",
    "        comments_processed = []\n",
    "        \n",
    "        for comment in comments[:self.max_comments]:\n",
    "            comment_text = comment['text']\n",
    "            comment_seq = self.text_to_sequence(comment_text)\n",
    "            comment_padded = self.pad_sequence(comment_seq, self.max_comment_len)\n",
    "            comments_processed.append(comment_padded)\n",
    "        \n",
    "        while len(comments_processed) < self.max_comments:\n",
    "            comments_processed.append([self.vocab['<PAD>']] * self.max_comment_len)\n",
    "        \n",
    "        return {\n",
    "            'content': torch.tensor(content_padded, dtype=torch.long),\n",
    "            'comments': torch.tensor(comments_processed, dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class RumorLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super(RumorLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.content_lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                                   batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        self.comment_lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                                   batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        self.content_attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=8)\n",
    "        self.comment_attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=8)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, content, comments):\n",
    "        batch_size = content.size(0)\n",
    "        num_comments = comments.size(1)\n",
    "        \n",
    "        content_embedded = self.embedding(content)\n",
    "        content_output, (content_hidden, _) = self.content_lstm(content_embedded)\n",
    "        \n",
    "        content_output = content_output.transpose(0, 1)  # (seq_len, batch, features)\n",
    "        content_attn_output, _ = self.content_attention(content_output, content_output, content_output)\n",
    "        content_attn_output = content_attn_output.transpose(0, 1)  # (batch, seq_len, features)\n",
    "        \n",
    "        content_pooled = torch.mean(content_attn_output, dim=1)  # (batch, features)\n",
    "        \n",
    "        comments_embedded = self.embedding(comments)  # (batch, num_comments, comment_len, embedding_dim)\n",
    "        \n",
    "        comments_reshaped = comments_embedded.view(batch_size * num_comments, -1, comments_embedded.size(-1))\n",
    "        comments_output, (comments_hidden, _) = self.comment_lstm(comments_reshaped)\n",
    "        \n",
    "        comments_output = comments_output.transpose(0, 1)\n",
    "        comments_attn_output, _ = self.comment_attention(comments_output, comments_output, comments_output)\n",
    "        comments_attn_output = comments_attn_output.transpose(0, 1)\n",
    "        \n",
    "        comments_pooled = torch.mean(comments_attn_output, dim=1)  # (batch*num_comments, features)\n",
    "        comments_pooled = comments_pooled.view(batch_size, num_comments, -1)  # (batch, num_comments, features)\n",
    "        \n",
    "        comments_aggregated = torch.mean(comments_pooled, dim=1)  # (batch, features)\n",
    "        \n",
    "        combined = torch.cat([content_pooled, comments_aggregated], dim=1)  # (batch, features*2)\n",
    "        \n",
    "        output = self.fc(combined)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class RumorDetector:\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=64, n_layers=2, dropout=0.3):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.vocab = None\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def build_vocab(self, texts, min_freq=2):\n",
    "        \"\"\"构建词汇表\"\"\"\n",
    "        word_counter = Counter()\n",
    "        \n",
    "        for text in texts:\n",
    "            words = jieba.lcut(text)\n",
    "            word_counter.update(words)\n",
    "        \n",
    "        vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        idx = 2\n",
    "        \n",
    "        for word, count in word_counter.items():\n",
    "            if count >= min_freq:\n",
    "                vocab[word] = idx\n",
    "                idx += 1\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        \"\"\"预处理数据\"\"\"\n",
    "        contents = []\n",
    "        comments_list = []\n",
    "        labels = []\n",
    "        all_texts = []\n",
    "        \n",
    "        for item in data:\n",
    "            content = item['content']\n",
    "            comments = item['comments']\n",
    "            \n",
    "            contents.append(content)\n",
    "            comments_list.append(comments)\n",
    "            labels.append(item['label'])\n",
    "            \n",
    "            all_texts.append(content['text'])\n",
    "            for comment in comments:\n",
    "                all_texts.append(comment['text'])\n",
    "        \n",
    "        return contents, comments_list, labels, all_texts\n",
    "    \n",
    "    def train(self, train_data, val_data=None, epochs=10, batch_size=32, learning_rate=0.001):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        train_contents, train_comments, train_labels, all_texts = self.preprocess_data(train_data)\n",
    "        \n",
    "        if self.vocab is None:\n",
    "            self.vocab = self.build_vocab(all_texts)\n",
    "        \n",
    "        train_dataset = RumorDataset(train_contents, train_comments, train_labels, self.vocab)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        vocab_size = len(self.vocab)\n",
    "        self.model = RumorLSTM(vocab_size, self.embedding_dim, self.hidden_dim, \n",
    "                              output_dim=2, n_layers=self.n_layers, dropout=self.dropout)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                content = batch['content'].to(self.device)\n",
    "                comments = batch['comments'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(content, comments)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            if val_data:\n",
    "                val_metrics = self.evaluate(val_data)\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, '\n",
    "                      f'Val Acc: {val_metrics[\"accuracy\"]:.4f}, Val F1: {val_metrics[\"f1\"]:.4f}')\n",
    "            else:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"评估模型\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet!\")\n",
    "        \n",
    "        test_contents, test_comments, test_labels, _ = self.preprocess_data(test_data)\n",
    "        test_dataset = RumorDataset(test_contents, test_comments, test_labels, self.vocab)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                content = batch['content'].to(self.device)\n",
    "                comments = batch['comments'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(content, comments)\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision = precision_score(all_labels, all_predictions)\n",
    "        recall = recall_score(all_labels, all_predictions)\n",
    "        f1 = f1_score(all_labels, all_predictions)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    def predict(self, content, comments):\n",
    "        \"\"\"预测单条数据\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet!\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        content_seq = self._text_to_sequence(content['text'])\n",
    "        content_padded = self._pad_sequence(content_seq, 200)\n",
    "        \n",
    "        comments_processed = []\n",
    "        for comment in comments[:100]:\n",
    "            comment_seq = self._text_to_sequence(comment['text'])\n",
    "            comment_padded = self._pad_sequence(comment_seq, 50)\n",
    "            comments_processed.append(comment_padded)\n",
    "        \n",
    "        while len(comments_processed) < 100:\n",
    "            comments_processed.append([self.vocab['<PAD>']] * 50)\n",
    "        \n",
    "        content_tensor = torch.tensor([content_padded], dtype=torch.long).to(self.device)\n",
    "        comments_tensor = torch.tensor([comments_processed], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model(content_tensor, comments_tensor)\n",
    "            prediction = torch.argmax(output, dim=1)\n",
    "            probability = torch.softmax(output, dim=1)\n",
    "        \n",
    "        return {\n",
    "            'prediction': '谣言' if prediction.item() == 1 else '非谣言',\n",
    "            'confidence': probability[0][prediction.item()].item()\n",
    "        }\n",
    "    \n",
    "    def _text_to_sequence(self, text):\n",
    "        words = jieba.lcut(text)\n",
    "        sequence = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
    "        return sequence\n",
    "    \n",
    "    def _pad_sequence(self, sequence, max_len):\n",
    "        if len(sequence) > max_len:\n",
    "            return sequence[:max_len]\n",
    "        else:\n",
    "            return sequence + [self.vocab['<PAD>']] * (max_len - len(sequence))\n",
    "    def save(self, filepath):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"请先创建模型!\")\n",
    "        \n",
    "        if self.vocab is None:\n",
    "            raise ValueError(\"Vocabulary not built yet! Cannot save model without vocabulary.\")\n",
    "\n",
    "        save_data = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'vocab': self.vocab,\n",
    "            'model_config': {\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'hidden_dim': self.hidden_dim,\n",
    "                'n_layers': self.n_layers,\n",
    "                'dropout': self.dropout,\n",
    "                'vocab_size': len(self.vocab)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        torch.save(save_data, filepath)\n",
    "        print(f\"模型已保存到: {filepath}\")\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"模型文件不存在: {filepath}\")\n",
    "        \n",
    "        save_data = torch.load(filepath, map_location=self.device)\n",
    "        \n",
    "        self.vocab = save_data['vocab']\n",
    "        \n",
    "        model_config = save_data['model_config']\n",
    "        self.embedding_dim = model_config['embedding_dim']\n",
    "        self.hidden_dim = model_config['hidden_dim']\n",
    "        self.n_layers = model_config['n_layers']\n",
    "        self.dropout = model_config['dropout']\n",
    "        \n",
    "        vocab_size = model_config['vocab_size']\n",
    "        self.model = RumorLSTM(vocab_size, self.embedding_dim, self.hidden_dim,output_dim=2, n_layers=self.n_layers, dropout=self.dropout)\n",
    "        \n",
    "        self.model.load_state_dict(save_data['model_state_dict'])\n",
    "        self.model.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25daa367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data():\n",
    "    \"\"\"创建示例数据\"\"\"\n",
    "    sample_data = [\n",
    "        {\n",
    "            'content': {\n",
    "                'text': '今天发生了一件重大事件，据说有外星人降临地球！',\n",
    "                'time': '2024-01-01 10:00:00'\n",
    "            },\n",
    "            'comments': [\n",
    "                {'text': '真的吗？太不可思议了！', 'time': '2024-01-01 10:05:00', 'name': '用户A', 'reply2': ''},\n",
    "                {'text': '这是谣言，不要相信', 'time': '2024-01-01 10:10:00', 'name': '用户B', 'reply2': ''},\n",
    "                {'text': '我也听说了，但还没有官方证实', 'time': '2024-01-01 10:15:00', 'name': '用户C', 'reply2': '用户A'}\n",
    "            ],\n",
    "            'label': 1\n",
    "        },\n",
    "        {\n",
    "            'content': {\n",
    "                'text': '市政府发布通知，明天全市停水检修',\n",
    "                'time': '2024-01-02 09:00:00'\n",
    "            },\n",
    "            'comments': [\n",
    "                {'text': '收到，谢谢提醒', 'time': '2024-01-02 09:05:00', 'name': '用户D', 'reply2': ''},\n",
    "                {'text': '官方已经确认了这个消息', 'time': '2024-01-02 09:10:00', 'name': '用户E', 'reply2': ''}\n",
    "            ],\n",
    "            'label': 0 \n",
    "        }\n",
    "    ]\n",
    "    return sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bb7b1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型...\n",
      "Epoch 1/5, Loss: 0.6645\n",
      "Epoch 2/5, Loss: 0.6451\n",
      "Epoch 3/5, Loss: 0.6261\n",
      "Epoch 4/5, Loss: 0.6033\n",
      "Epoch 5/5, Loss: 0.5989\n",
      "\n",
      "评估模型...\n",
      "测试集结果: 准确率: 0.0000, F1分数: 0.0000\n",
      "\n",
      "保存模型...\n",
      "模型已保存到: ./model.pth\n",
      "\n",
      "预测新数据...\n",
      "预测结果: 谣言, 置信度: 0.5700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\sjmt\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "data = create_sample_data()\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "detector = RumorDetector()\n",
    "\n",
    "print(\"开始训练模型...\")\n",
    "detector.train(train_data, epochs=5, batch_size=2, learning_rate=0.001)\n",
    "\n",
    "print(\"\\n评估模型...\")\n",
    "metrics = detector.evaluate(test_data)\n",
    "print(f\"测试集结果: 准确率: {metrics['accuracy']:.4f}, F1分数: {metrics['f1']:.4f}\")\n",
    "print(\"\\n保存模型...\")\n",
    "filepath = \"./model.pth\"\n",
    "detector.save(filepath)\n",
    "\n",
    "print(\"\\n预测新数据...\")\n",
    "new_content = {'text': '最新消息：明天会有特大暴雨，建议居家办公', 'time': '2024-01-03 08:00:00'}\n",
    "new_comments = [\n",
    "    {'text': '气象局已经辟谣了，这是假消息', 'time': '2024-01-03 08:05:00', 'name': '用户F', 'reply2': ''},\n",
    "    {'text': '不要传播不实信息', 'time': '2024-01-03 08:10:00', 'name': '用户G', 'reply2': ''}\n",
    "]\n",
    "\n",
    "result = detector.predict(new_content, new_comments)\n",
    "print(f\"预测结果: {result['prediction']}, 置信度: {result['confidence']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7869dc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果: 谣言, 置信度: 0.5700\n"
     ]
    }
   ],
   "source": [
    "detector = RumorDetector()\n",
    "detector.load(filepath)\n",
    "result = detector.predict(new_content, new_comments)\n",
    "print(f\"预测结果: {result['prediction']}, 置信度: {result['confidence']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sjmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
